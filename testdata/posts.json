{"items":[{"abstract":null,"archive_url":null,"authors":[{"name":"Research Graph"}],"blog":{"api":false,"archive_prefix":null,"authors":null,"backlog":0,"canonical_url":null,"category":"computerAndInformationSciences","created_at":1706685423,"current_feed_url":null,"description":"Stories by Research Graph on Medium","favicon":"https://cdn-images-1.medium.com/fit/c/150/150/1*laJi0jBkVoGhXid7gD_DmQ.png","feed_format":"application/rss+xml","feed_url":"https://medium.com/@researchgraph/feed","filter":null,"funding":null,"generator":"Medium","generator_raw":"Medium","home_page_url":"https://medium.com/@researchgraph","id":"30da2ca9-8258-4ab5-acca-3919d9a5d98d","indexed":true,"issn":null,"language":"en","license":"https://creativecommons.org/licenses/by/4.0/legalcode","mastodon":"","plan":"Starter","prefix":"10.59350","relative_url":null,"ror":null,"secure":true,"slug":"researchgraph","status":"active","title":"Stories by Research Graph on Medium","updated_at":1708297648,"use_api":null,"use_mastodon":false,"user_id":"a7e16958-1175-437c-b839-d4b8a47ec811","version":"https://jsonfeed.org/version/1.1"},"blog_name":"Stories by Research Graph on Medium","blog_slug":"researchgraph","content_text":"**The AI Helper Turning Mountains of Data into Bite-Sized Instructions**\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1000/0*XwvfCyjF7dUWQ7Wj\" />\n<figcaption>Image generated with Google\u2019s Gemini\n12\u201302\u20132024.</figcaption>\n</figure>\n\nAuthor: Aland Astudillo <https://orcid.org/0009-0008-8672-3168>\n\nLLMs have been changing the way the entire world deals with problems and\nday-by-day tasks. To make them better for specific applications, they\nneed huge amounts of data and complex and expensive approaches to\ntraining them. However, there are some challenges such as limited prompt\nsize, and limited context windows that make LLM not suitable for some.\nThis is a major issue for tasks that require huge amounts of\ninformation. LLMLingua has been developed as a framework that helps LLMs\nin addressing these limitations. In this article, we will review what\nLLMLingua is, what it does, how it does it and what we can expect in the\nnear\u00a0future.\n\nPicture this scenario, you're tasked with teaching a group of aspiring\nchefs how to prepare a complex gourmet meal. You could throw every\nrecipe detail and culinary term at them, hoping they absorb it all. But\nwouldn't it be more effective to break down the instructions into clear,\nconcise steps, focusing only on the crucial techniques and ingredients?\nThis, in essence, is the magic of LLMLingua.\n\nInstead of aspiring chefs, imagine training a team of AI apprentices to\ntackle complex tasks. Like those aspiring chefs who thrive in learning\nto prepare new dishes, LLMs thrive on information. But feeding LLMs with\nmountains of raw data can be overwhelming, leading to slow processing\nand limited performance. Enter LLMLingua, the innovative tool acting as\nthe AI chef, meticulously preparing information into bite-sized\ninstructions that these AI apprentices can easily digest and\u00a0master.\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1000/0*WauCih49veOTPy9V\" />\n<figcaption>Image generated with Google\u2019s Gemini\n12\u201302\u20132024.</figcaption>\n</figure>\n\nSo, how does LLMLingua craft this culinary\u00a0magic?\n\nIt employs two key techniques, similar to how a skilled chef optimises\na\u00a0recipe:\n\n1.  **Ingredient Trimming:** Imagine each piece of information as an\n    ingredient in the recipe. LLMLingua, akin to a seasoned chef,\n    identifies and discards unnecessary elements like filler words or\n    irrelevant details. This \"trimming\" streamlines instructions,\n    keeping only the essential components, just like using only the\n    right spices to enhance a dish's\u00a0flavour.\n2.  **Recipe Refining**: Even carefully chosen ingredients need precise\n    instructions. LLMLingua refines the wording and phrasing of\n    instructions, ensuring the AI apprentices grasp the core meaning\n    with perfect clarity. It's like the chef rewriting the recipe in\n    clear, concise steps, leaving no room for confusion or\n    misinterpretation.\n\nCurrently, several variations of LLMLingua exist, each tailored to\nspecific tasks and LLM architectures. One version optimises prompts for\nquestion-answering models, while another focuses on improving\nsummarization tasks. These variations highlight the adaptability and\nevolving nature of this technology.\n\nBut LLMLingua isn't alone in the quest for efficient AI. Techniques like\nknowledge distillation and parameter reduction also aim to streamline\nmodels. What sets LLMLingua apart is its focus on prompt manipulation,\noffering a unique and flexible approach.\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1000/0*hAjLbEGPdLOJyziQ\" />\n<figcaption>Scheme of the LLMLingua framework. Image created by Aland\nAstudillo.</figcaption>\n</figure>\n\nImagine teaching a friend a complex recipe. You wouldn't overwhelm them\nwith every detail; instead, you'd break it down, focusing on key steps\nand ingredients. LLMLingua operates similarly, streamlining information\nfor large language models (LLMs) to achieve peak performance. Here's a\ndeeper dive into its technical pipeline:\n\n1.  **Input Preprocessing**: The journey begins with the \"raw\"\n    information intended for the LLM. LLMLingua analyses it, identifying\n    irrelevant elements like filler words or redundant instructions.\n    Think of skimming unnecessary details from a recipe book, keeping\n    only the crucial steps. This initial \"trimming\" helps reduce the\n    information load on the\u00a0LLM.\n2.  **Tokenization**: Next, LLMLingua breaks down the preprocessed text\n    into individual units called \"tokens,\" similar to words in a\n    sentence. This facilitates further analysis and manipulation.\n    Imagine separating the ingredients listed in your recipe into\n    individual units\u200a---\u200aflour, eggs, milk,\u00a0etc.\n3.  **Budget Control**: LLMLingua employs a \"budget controller\" to\n    ensure compression doesn't compromise information integrity. By\n    setting a desired compression ratio, it controls how much\n    information can be discarded while maintaining optimal performance\n    for the LLM. This is like deciding how much of each ingredient to\n    use without altering the final dish drastically.\n4.  **Iterative Compression**: Now comes the magic. LLMLingua utilises\n    an iterative algorithm to refine the tokenized information. Each\n    iteration analyses the remaining tokens, identifying opportunities\n    for further compression while considering their importance for the\n    LLM's task. Think of repeatedly reviewing your recipe steps,\n    replacing complex techniques with simpler alternatives whenever\n    possible.\n5.  **Instruction Tuning**: Beyond simply removing elements, LLMLingua\n    also refines the wording and phrasing of the remaining instructions.\n    Imagine rewriting your recipe steps for clarity and accuracy,\n    ensuring your friend understands them perfectly. This ensures the\n    LLM receives clear and concise instructions, minimising room for\n    misinterpretation.\n6.  **Output Generation**: Finally, the compressed and refined\n    information is presented to the LLM as a \"streamlined recipe.\" The\n    LLM processes it efficiently, achieving higher performance with\n    faster response times and reduced resource consumption. It's like\n    your friend effortlessly executing the optimised recipe, producing a\n    delicious dish efficiently.\n\nThis is just a simplified overview and the actual LLMLingua pipeline\ninvolves complex algorithms and deep learning techniques that can be\nreviewed in the research article (See the references).\n\n**Key concepts: Entropy and perplexity**\n\nAdditionally, there are two key concepts that allow the control engine\nin the LLMLingua framework: entropy and perplexity. These metrics act as\nindicators of information content and difficulty, allowing researchers\nto gauge the effectiveness of LLMLingua's compression techniques.\n\n**1. Entropy**\n\nImagine tossing a coin. With one possible outcome (heads or tails), the\nentropy is low. Now, consider a hundred-sided die\u200a---\u200awith many more\npossibilities\u200a---\u200athe entropy is higher. Similarly, entropy in language\nmeasures the unpredictability of the next word given the previous ones.\nLonger, more complex sentences typically have higher entropy than\nshorter, simpler\u00a0ones.\n\nLLMLingua aims to reduce the entropy of instructions fed to LLMs. By\nremoving redundant information and focusing on key elements, it\nessentially makes the next word more predictable, like simplifying the\ndie to fewer sides. This lowers the information load on the LLM, leading\nto more efficient processing.\n\n**2. Perplexity**\n\nPerplexity builds upon entropy but is presented as an inverse\nprobability. A lower perplexity value signifies higher predictability,\nindicating that the model can more easily anticipate the next word.\nConversely, a high perplexity suggests complex, unpredictable language,\nmaking it harder for the model to\u00a0process.\n\nIn the context of LLMLingua, a decrease in perplexity after compression\nreflects improved efficiency. It means the LLM can understand the\ncompressed instructions with the same accuracy as the original but with\nless effort. This translates to faster response times and lower\ncomputational costs.\n\nIt's important to note\u00a0that:\n\n- Both entropy and perplexity are complex measures with nuances beyond\n  this simplified explanation.\n- LLMLingua's goal isn't to achieve the absolute lowest entropy or\n  perplexity, but to find the optimal balance between compression and\n  information fidelity.\n\nSo, while entropy and perplexity might seem like abstract concepts, they\nplay a crucial role in understanding how LLMLingua achieves its\nefficiency gains.\n\n**Why is LLMLingua the secret ingredient for AI\u00a0success?**\n\nLLMs are changing the world, translating languages on the fly, composing\npersonalised poems, and even answering your questions with remarkable\naccuracy. But just like our aspiring chefs, their hunger for information\ncan create limitations. Imagine translating entire novels\nword-for-word\u200a---\u200ait's slow, resource-intensive, and ultimately\nunsatisfying. LLMLingua tackles this head-on, paving the way for faster,\nmore efficient, and impactful AI applications:\n\n- **Speedy Chatbots**: Ever feel like waiting forever for a chatbot\n  response? LLMLingua compresses your questions, enabling chatbots to\n  understand you instantly and respond at lightning speed. It's like\n  having a personal AI assistant who's always on top of their game,\n  eliminating frustrating wait\u00a0times.\n- **Translation on the Go**: Imagine translating entire documents in\n  seconds! LLMLingua empowers translation tools to process information\n  with laser precision, breaking down language barriers faster than\n  ever. Think of it as a universal translator, seamlessly bridging\n  communication gaps across cultures and languages like a skilled\n  multilingual chef seamlessly navigating different cuisines.\n- **Research Revolution**: Picture sifting through mountains of\n  scientific data in minutes. LLMLingua empowers AI assistants to\n  analyse complex research papers with unmatched efficiency,\n  accelerating scientific breakthroughs and discoveries. It's like\n  having a tireless research partner who can summarise vast amounts of\n  information, highlighting key findings and saving researchers\n  countless hours.\n- **AI for Everyone**: From personalised learning experiences to\n  efficient task management, LLMLingua can revolutionise how AI\n  assistants interact with us. Imagine an AI tutor who personalises\n  learning modules based on your individual needs or an assistant who\n  manages your schedule with laser-focus, all thanks to the power of\n  concise and optimised instructions.\n\nBut, wait...how do we solve the problem of the limited context size? Let\nme introduce you to **LongLLMLingua**!\n\n### What is LongLLMLingua?\n\nThink of LongLLMLingua as the master chef, overseeing the entire\nculinary experience. It builds upon LLMLingua's foundation, applying its\ncompression techniques not just to single instructions, but to entire\nsequences of information. This empowers LLMs to process long\ncontexts\u200a---\u200athink research papers, dialogue history, or complex\nnarratives\u200a---\u200awith impressive efficiency and improved performance.\n\n**How does it\u00a0work?**\n\nLongLLMLingua operates in three key\u00a0phases:\n\n- **Coarse Compression**: First, it performs a \"rough cut,\" analysing\n  the entire context and identifying large sections it can safely\n  discard. Imagine skimming extraneous details from a recipe book,\n  focusing only on the core steps for each\u00a0dish.\n- **Fine Compression**: Next, it dives deeper, applying LLMLingua's\n  techniques to refine remaining information. Think of meticulously\n  preparing each dish, ensuring optimal ingredient selection and precise\n  instructions.\n- **Reordering for Coherence**: Finally, LongLLMLingua ensures the\n  compressed information maintains its original meaning and flow.\n  Imagine plating each dish in a visually appealing and sequential\n  manner, ensuring a cohesive dining experience.\n\n**Why is LongLLMLingua important?**\n\nLong contexts are crucial in various AI applications:\n\n- **Question Answering**: Imagine needing context from several articles\n  to answer a complex question. LongLLMLingua helps LLMs retain key\n  information across documents, leading to more accurate\u00a0answers.\n- **Machine Translation**: Imagine translating entire books instead of\n  single sentences. LongLLMLingua ensures coherence and preserves\n  meaning despite lengthy\u00a0input.\n- **Dialogue Systems**: Imagine chatbots understanding intricate\n  conversation history. LongLLMLingua enables them to maintain context,\n  leading to more natural and engaging interactions.\n\nReal-world examples of LongLLMLingua in\u00a0action:\n\n- **LongBench Benchmark**: This benchmark measures LLM performance in\n  long context scenarios. When applied to GPT-3.5-Turbo, LongLLMLingua\n  achieved a 17.1% performance boost with 4x fewer\u00a0tokens.\n- **ZeroScrolls Benchmark**: This benchmark focuses on reading\n  comprehension in long contexts. LongLLingua reduced costs by \\$27.4\n  per 1,000 samples while maintaining performance.\n\nRemember, LongLLMLingua isn't limited to cooking analogies. It's a\npowerful tool revolutionising how LLMs handle long contexts,\naccelerating progress in various AI domains. By improving efficiency and\nmaintaining information fidelity, LongLLMLingua opens doors to a future\nwhere AI interactions are more contextual, seamless, and impactful.\n\n**Conclusion**\n\nIn conclusion, both LLMLingua and LongLLMLingua are not just\ntechnological advancements; they're culinary metaphors come to life.\nLLMLingua acts as the skilled chef, meticulously preparing information\ninto bite-sized instructions for AI apprentices. LongLLMLingua takes the\nbaton, transforming into the master chef, seamlessly handling entire\nsequences of information like preparing a multi-course feast. Together,\nthey're revolutionising how LLMs interact with information, paving the\nway for a future\u00a0where:\n\n- AI responses are faster, more accurate, and contextually relevant.\n- Machine translation transcends sentence-by-sentence limitations,\n  unlocking global communication on a grand\u00a0scale.\n- AI assistants understand our interactions in their entirety, leading\n  to more natural and engaging dialogues.\n\nAs these technologies continue to evolve, the possibilities are endless.\nImagine AI tutors adapting to your learning style based on years of\neducational data, or chatbots understanding months of conversation\nhistory to anticipate your needs with uncanny accuracy.\n\nThe future of AI is hungry for efficiency and understanding, and\nLLMLingua and LongLLMLingua are serving up the perfect recipe for\nsuccess. By empowering LLMs to process information like culinary\nmasters, they're opening doors to a new era of seamless interaction,\naccelerated discovery, and boundless communication.\n\n**Sources**\n\n- LLMLingua research paper: Jiang et al., 2023. LLMLingua: Compressing\n  prompts for accelerated inference of Large Language Models. arXiv\n  2310.05736v2 <https://arxiv.org/abs/2310.05736>\n- LongLLMLingua research paper: Jiang et al., 2023. LongLLMLingua:\n  accelerating and enhancing LLMs in long context scenarios via prompt\n  compression. arXiv 2310.06839v1 <https://arxiv.org/abs/2310.06839>\n- Microsoft LLMLingua page: LLMLingua: Compressing Prompts for\n  Accelerated Inference of Large Language Models\u200a---\u200aMicrosoft Research\n  <https://www.microsoft.com/en-us/research/project/llmlingua/>\n- Hugging Face LLMLingua Space: LLMLingua\u200a---\u200aa Hugging Face\n  <https://huggingface.co/spaces/microsoft/LLMLingua>\n\n![](https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3a632385b206){width=\"1\"\nheight=\"1\"}\n","doi":"https://doi.org/10.59350/2k8wt-8kr16","guid":"https://medium.com/p/3a632385b206","id":"014a7016-8a9a-4e68-94b4-32e1acf60552","image":null,"indexed_at":1,"language":"en","published_at":1708297648,"reference":[],"relationships":[],"summary":"<strong>\n The AI Helper Turning Mountains of Data into Bite-Sized Instructions\n</strong>\nAuthor: Aland Astudillo https://orcid.org/0009-0008-8672-3168 LLMs have been changing the way the entire world deals with problems and day-by-day tasks. To make them better for specific applications, they need huge amounts of data and complex and expensive approaches to training them.","tags":["Llmlingua","Nlp","Retrieval-augmented","Llm","Artificalintelligence"],"title":"What is LLMLingua?","updated_at":1708297648,"url":"https://medium.com/@researchgraph/what-is-llmlingua-3a632385b206"},{"abstract":null,"archive_url":null,"authors":[{"name":"Research Graph"}],"blog":{"api":false,"archive_prefix":null,"authors":null,"backlog":0,"canonical_url":null,"category":"computerAndInformationSciences","created_at":1706685423,"current_feed_url":null,"description":"Stories by Research Graph on Medium","favicon":"https://cdn-images-1.medium.com/fit/c/150/150/1*laJi0jBkVoGhXid7gD_DmQ.png","feed_format":"application/rss+xml","feed_url":"https://medium.com/@researchgraph/feed","filter":null,"funding":null,"generator":"Medium","generator_raw":"Medium","home_page_url":"https://medium.com/@researchgraph","id":"30da2ca9-8258-4ab5-acca-3919d9a5d98d","indexed":true,"issn":null,"language":"en","license":"https://creativecommons.org/licenses/by/4.0/legalcode","mastodon":"","plan":"Starter","prefix":"10.59350","relative_url":null,"ror":null,"secure":true,"slug":"researchgraph","status":"active","title":"Stories by Research Graph on Medium","updated_at":1708297648,"use_api":null,"use_mastodon":false,"user_id":"a7e16958-1175-437c-b839-d4b8a47ec811","version":"https://jsonfeed.org/version/1.1"},"blog_name":"Stories by Research Graph on Medium","blog_slug":"researchgraph","content_text":"### How to use GROBID to extract text from\u00a0PDF\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1024/1*uyD7hkSBZGCOn4wqu2TgMw.png\" />\n<figcaption>Created using DALL.E on 13 Feb\u00a02024</figcaption>\n</figure>\n\nAuthor: Aland Astudillo <https://orcid.org/0009-0008-8672-3168>\n\nGROBID is a powerful and useful tool based on machine learning that can\nextract text information from PDF files and other files to a structured\nformat.\n\nOne of the key challenges in knowledge mining from academic articles is\nreading the content of PDF files. The ability to efficiently and\naccurately read the content of PDF documents using automated tools and\ntransform this data into a structured model provides many opportunities\nfor the integration of PDF collections to expert systems, and\nAI-assisted data pipelines such as chatbots.\n\nIn this article, we will discuss GROBID capabilities, explain how to\ninstall it, and explore how to use it in a Python script or Jupyter\nNotebook.\n\n### **What is\u00a0GROBID?**\n\nGROBID stands for GeneRation of Bibliographic Data and is a machine\nlearning library for extracting, parsing, and re-structuring raw\ndocuments such as PDF into structured XML/TEI encoded documents. It has\na particular focus on technical and scientific publications. TEI stands\nfor Text Encoding Initiative (<https://tei-c.org/Guidelines/>). It\ndefines and documents a markup language for representing the structural,\nrenditional, and conceptual features of\u00a0texts.\n\n### How it\u00a0works?\n\nGROBID is a machine-learning library that extracts information from PDF\nfiles. The goal of GROBID is to help the text mining process,\ninformation extraction, and semantic analysis of scientific publications\nby transforming them into machine-friendly, structured, representations\nwith predictable structure.\n\nDespite the large amount of PDF files and their extensive use among all\nfields, it is still a big challenge to process them because they are\nencoded in a vast variety of different publisher formats, often\nincomplete or inconsistent. In short, PDF files are difficult to exploit\nand\u00a0use.\n\nIn a nutshell, GROBID uses a cascade of \"sequence labelling models\" to\nparse a document. Each model from the collection of models is in charge\nof identifying a small amount of labels to classify areas and\ninformation in the document. The final models produce 55 labels where\neach piece of identified information from the document will be\nstructured. More information about how this library has been developed\nand the machine learning models are in this link:\n<https://grobid.readthedocs.io/en/latest/Principles/>\n\n### How to install\u00a0GROBID\n\nHere we explain the installation of the GROBID tool as a service in a\nLinux machine (Ubuntu for example). The same steps apply to a Windows\nsystem using the Windows Subsystem for Linux tool (WSL version\u00a02).\n\nFor Linux, in the terminal with super user (sudo) rights create a folder\nand once inside, proceed to download the last version of GROBID using\nthe wget tool. The current version of GROBID is v 0.8.0. After that,\nunzip the\u00a0file:\n\n    > wget https://github.com/kermitt2/grobid/archive/0.8.0.zip\n    > unzip 0.8.0.zip\n\nFor Windows, you can use Docker containers to install and run the GROBID\nservice locally. More documentation is available at\n<https://grobid.readthedocs.io/en/latest/Run-Grobid/>\n\nNote: At the time of writing this article, GROBID requires JAVA(JDK) and\nit works fine with JDK version 1.11 up to version JDK 1.17. Other recent\nJDK versions should work correctly. To check your JAVA installation, in\nthe terminal use the following command:\n\n    > java -version\n\nIf you do not have any JAVA installed, proceed to install JAVA using the\nfollowing line in the terminal.\n\n    > sudo apt install default-jdk\n\nIt is a good practice to check Java after installation to ensure the\nright version of Java is installed and ready to operate on your\u00a0system.\n\n### **How to run\u00a0GROBID**\n\nOnce you downloaded and unzipped the \"*grobid\"* file, proceed to the\nfolder and run the grewed file. In the terminal ensure to use \"./\"\nbefore the command \"*grewed\".*\n\n    > cd grobid-0.8.0\n\n    > ./grewed run\n\nThe first time that you run the service, it will download and install\nall the required dependencies and then it will run the service. It will\nbe completed by printing a message \"**:grobid-service:run\"** as\npresented in the following screenshot.\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/920/1*gEsGJ87IUzEovXwcFRDCzg.png\" />\n</figure>\n\n### **How to use\u00a0GROBID**\n\nAfter running the GROBID service, you can connect to the created server\nin the following direction using your\u00a0browser:\n\n<http://localhost:8070/>\n\nThis follows the default configuration, but you can change the port in\nthe config file. The image below shows how it looks the first time that\nyou access the web\u00a0layout.\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1003/1*N5URo-rzuOb_1Cu15yEKEA.png\" />\n</figure>\n\nThe TEI menu will allow you to choose among the different services to\ncall and to upload a file with the button \"Select file\". The following\nfunctionalities are available as a\u00a0service:\n\n- Header extraction and\u00a0parsing\n- References extraction and\u00a0parsing\n- Citation contexts recognition and resolution\n- Full-text extraction and structuring\n- PDF coordinates\n- Parsing of references\n- Parsing of\u00a0names\n- Parsing of affiliation and\u00a0address\n- Parsing of\u00a0dates\n- Consolidation and resolution of the extracted bibliographical\n  references\n- Extraction and parsing of patent and non-patent references in patent\n  publications\n- Extraction of funders and funding information\n\nOnce you upload a PDF document and choose the required service, you can\npress the Submit\u00a0button\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/995/1*m3zDNnvFesFQuQ7Bv0U3yA.png\" />\n</figure>\n\nAfter a few seconds, the web will show you the XML structure of the\noutput depending on what service you have selected. You can download the\nresulting XML structure in an XML file. The image below shows the result\nof processing the full-text document for a given paper. For example, in\nthe image below we used the article *Pan et al., 2024 Unifying Large\nLanguage Models and Knowledge Graphs: A Roadmap. arXiv:2306.08302*\ndownloaded from arXiv (<https://arxiv.org/abs/2306.08302>)\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/993/1*xTfpktk_4nH1dOPqoETvkg.png\" />\n</figure>\n\n### **How to use GROBID Python\u00a0Client**\n\nThe GROBID Python client is a handy tool that can process efficiently\nand concurrently a set of PDF files in a given folder using GROBID as a\nservice without requiring the web option. It includes a command line for\nprocessing PDFs and writing the results in a given folder. It allows us\nto use this tool inside Python scripts. To use this client, It assumes\nthat you are running GROBID service, as we saw in the previous\u00a0steps.\n\nThe repository of this client is here\n<https://github.com/kermitt2/grobid_client_python/blob/master/grobid_client/grobid_client.py>\n\nTo install the GROBID Python client to use in your scripts or Jupyter\nNotebook, in your Python environment, you can use the pip manager in a\ngiven environment:\n\n    !pip install grobid_client_python\n\nIf you want to use this client in a script or a Jupyter Notebook, you\nmust import the GROBID client class. Remember that it assumes that the\nGROBID service is already\u00a0running.\n\n    from grobid_client.grobid_client import GrobidClient\n\n    client = GrobidClient(config_path=\"./config.json\")\n\nOnce the client connects to the service it will show you the following\nmessage.\n\n    \"GROBID server is up and running\"\n\n### **How to analyse a bunch of PDF documents**\n\nAfter checking the connection, you can call the process method to\nanalyse a bunch of documents in a folder. The process method has 3\nrequired parameters and other optional parameters. In the following\nexample we are using the service \"**processFulltextDocument**\" for\nprocessing all the pdf files in the folder \"**resources/test_pdf**\" and\nget the xml files output on the folder \"**resources/test_out/**\".\nAdditionally, we set the consolidate citation option and the TEI\ncoordinates to\u00a0\"true\".\n\n    client.process(\"processFulltextDocument\", \n         \"./resources/test_pdf\", \n     output=\"./resources/test_out/\", \n     consolidate_citations=True, \n     tei_coordinates=True, \n     force=True)\n\nIn the defined folder, the output will be the XML files for each PDF\nfile with the re-structured information.\n\n### **How to analyse a single PDF\u00a0file**\n\nFor analysing one PDF file at a time, you can use the method\n**process_pdf**, with similar parameters as the **process**\u00a0method:\n\n    service_name = \"processFulltextDocument\"\n\n    pdf_file = \"./pdf_examples/input/0046d83a-edd6-4631-b57c-755cdcce8b7f.pdf\"\n\n    rsp = client.process_pdf(service_name, pdf_file, \n                             generateIDs=True, \n                             consolidate_header=True, \n                             consolidate_citations=True, \n                             include_raw_citations=True, \n                             include_raw_affiliations=True, \n                             tei_coordinates=True, \n                             segment_sentences=True)\n\nThe following text snippet shows the top section of the \"rsp\" variable\nthat holds the results from the PDF processing.\n\n    ('./pdf_examples/input/0046d83a-edd6-4631-b57c-755cdcce8b7f.pdf',\n     200,\n     '<?xml version=\"1.0\" encoding=\"UTF-8\"?> \n    <TEI xml:space=\"preserve\" xmlns=\"http://www.tei-c.org/ns/1.0\"  \n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"  \n    xsi:schemaLocation=\"http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd\"  \n    xmlns:xlink=\"http://www.w3.org/1999/xlink\"> \n      <teiHeader xml:lang=\"en\"> \n        <fileDesc> \n          <titleStmt> \n            <title level=\"a\" type=\"main\" xml:id=\"_nrS3kCw\">Multi-contact functional electrical stimulation for hand opening: electrophysiologically driven identification of the optimal stimulation site</title> \n            <funder ref=\"#_ZTFTHCu\"> \n              <orgName type=\"full\">German Federal Ministry for Education and Research [BMBF</orgName> \n            </funder> \n            <funder ref=\"#_xTkqs2w\"> \n              <orgName type=\"full\">German Research Council</orgName> \n            </funder> \n            <funder ref=\"#_dV4wQd4 #_dVJJnCd\"> \n              <orgName type=\"full\">Federal Ministry for Education and Research</orgName> \n            </funder> \n          </titleStmt> \n          <publicationStmt> \n            <publisher>Springer Science and Business Media LLC</publisher> \n            <availability status=\"unknown\">\n       <p>Copyright Springer Science and Business Media LLC</p> \n            </availability> \n            <date type=\"published\" when=\"2016-03-08\">2016-03-08</date> \n          </publicationStmt> \n          <sourceDesc> \n            <biblStruct> \n              <analytic> \n                <author> \n                  <persName coords=\"1,56.69,234.15,85.88,9.71\"><forename type=\"first\">Cristiano</forename>\n                  <surname>De Marchis</surname></persName> \n                  <email>cristiano.demarchis@uniroma3.it</email> \n                  <affiliation key=\"aff0\"> \n                    <note type=\"raw_affiliation\"><label>1</label> Division of Functional and Restorative Neurosurgery, Department of Neurosurgery, Eberhard Karls University, Otfried-Mueller-Str.45, 72076 T\u00fcbingen, Germany</note> \n                    <orgName type=\"department\" key=\"dep1\">Division of Functional and Restorative Neurosurgery</orgName> \n                    <orgName type=\"department\" key=\"dep2\">Department of Neurosurgery</orgName> \n                    <orgName type=\"institution\">Eberhard Karls University</orgName> \n                    <address> \n                      <addrLine>Otfried-Mueller-Str.45</addrLine> \n                      <postCode>72076</postCode> \n                      <settlement>T\u00fcbingen</settlement> \n                      <country key=\"DE\">Germany</country> \n                    </address> \n                  </affiliation> \n                </author> \n    \u2026\n\n(Note: The result was truncated)\n\nAs you can see, the information is organised hierarchically, using\nstandardised tags \\<tag\\> to identify each type of information of the\npublication. For instance, the tag *\\<titleStmt\\>* contains sub-tags\nholding the title of the\u00a0article.\n\n    <title level=\"a\" type=\"main\" xml:id=\"_nrS3kCw\">\n    Multi-contact functional electrical stimulation for hand opening: electrophysiologically driven identification of the optimal stimulation site\n    </title> \n\nOther tags contain information about authors, affiliations, and so on.\nFor example, the identification of the organisation is in the tag\n*\\<orgName\\>*\n\n    <orgName type=\"full\">\n    German Federal Ministry for Education and Research [BMBF\n    </orgName> \n\nInformation about the authors is inside the tags *\\<author\\>* containing\ndetails of each author, complete name (\\<forename), last name\n(*\\<surname\\>*), email (*\\<email\\>*), affiliations (*\\<affiliation\nkey=\"aff0\\\"\\>*), and all other information like the address of the\ninstitution to which the author is affiliated (*\\<address\\>*), and all\nthe information contained in the article about the\u00a0author.\n\n    <author> \n      <persName coords=\"1,56.69,234.15,85.88,9.71\">\n        <forename type=\"first\">Cristiano</forename>\n        <surname>De Marchis</surname>\n      </persName> \n      <email>cristiano.demarchis@uniroma3.it</email> \n      <affiliation key=\"aff0\"> \n      <note type=\"raw_affiliation\"><label>1</label> Division of Functional and Restorative Neurosurgery, Department of Neurosurgery, Eberhard Karls University, Otfried-Mueller-Str.45, 72076 T\u00fcbingen, Germany</note> \n        <orgName type=\"department\" key=\"dep1\">Division of Functional and Restorative Neurosurgery</orgName> \n        <orgName type=\"department\" key=\"dep2\">Department of Neurosurgery</orgName> \n        <orgName type=\"institution\">Eberhard Karls University</orgName> \n        <address> \n          <addrLine>Otfried-Mueller-Str.45</addrLine> \n          <postCode>72076</postCode> \n          <settlement>T\u00fcbingen</settlement> \n          <country key=\"DE\">Germany</country> \n        </address> \n      </affiliation> \n    </author>\n\nIn the same way, we can retrieve information on the citations of the\narticle in the tag for references. For example, an article in the list\nof references has the following structure\n\n    <biblStruct coords=\"8,319.75,636.26,214.89,6.62;8,319.75,645.27,210.17,6.62;8,319.75,654.28,118.20,6.62\" xml:id=\"b0\">\n      <analytic>\n        <title level=\"a\" type=\"main\">The influence of functional electrical stimulation on hand motor recovery in stroke patients: a review</title>\n        <author>\n          <persName xmlns=\"http://www.tei-c.org/ns/1.0\">\n            <forename type=\"first\">Fanny</forename>\n            <surname>Quandt</surname>\n          </persName>\n        </author>\n        <author>\n          <persName xmlns=\"http://www.tei-c.org/ns/1.0\">\n            <forename type=\"first\">Friedhelm</forename>\n            <forename type=\"middle\">C</forename>\n            <surname>Hummel</surname>\n          </persName>\n        </author>\n        <idno type=\"DOI\">10.1186/2040-7378-6-9</idno>\n      </analytic>\n      <monogr>\n        <title level=\"j\">Experimental & Translational Stroke Medicine</title>\n        <title level=\"j\" type=\"abbrev\">Exp & Trans Stroke Med</title>\n        <idno type=\"ISSNe\">2040-7378</idno>\n        <imprint>\n          <biblScope unit=\"volume\">6</biblScope>\n          <biblScope unit=\"issue\">1</biblScope>\n          <biblScope unit=\"page\">9</biblScope>\n          <date type=\"published\" when=\"2014-08-21\"/>\n          <publisher>Springer Science and Business Media LLC</publisher>\n        </imprint>\n      </monogr>\n    </biblStruct>\n\nIn which we can identify the title tag (*\\<title level=\"a\"\ntype=\"main\"\\>*), authors information (*\\<author\\>*), DOI (*\\<idno\ntype=\"DOI\"\\>*), and the journal details (*\\<monogr\\>*) of the referenced\narticle.\n\nFinally, to get each piece of information from this structure for\nfurther analysis, the next steps involves extracting the required\ninformation based on the tags of the XML structure, which can be done\nwith different tools to match these tags in the text. That said, the\ntext is ready for mining all the information.\n\n### **Conclusion**\n\nWe have explored the important aspects of GROBID. With its simple\ninstallation process user interface, and scripts, we can put GROBID to\nwork easier than ever. Whether you are facing mountains of PDFs or\nstruggling to tame unruly bibliographic data, GROBID is your trusty\ncompanion on the path to research efficiency. Download GROBID today and\nunlock the door to a world of streamlined workflows, saving time, and\ndeeper insights from your research endeavours.\n\n**Materials for further\u00a0reading:**\n\n- <https://grobid.readthedocs.io/en/latest/>\n- <https://github.com/kermitt2/grobid_client_python>\n\n![](https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=67df995b16fa){width=\"1\"\nheight=\"1\"}\n","doi":"https://doi.org/10.59350/hz1er-vrh59","guid":"https://medium.com/p/67df995b16fa","id":"007fd5a5-75a2-4d6a-bd3b-21e0d7c08703","image":null,"indexed_at":1,"language":"en","published_at":1707773792,"reference":[],"relationships":[],"summary":"<strong>\n How to use GROBID to extract text from\u00a0PDF\n</strong>\nAuthor: Aland Astudillo https://orcid.org/0009-0008-8672-3168 GROBID is a powerful and useful tool based on machine learning that can extract text information from PDF files and other files to a structured format. One of the key challenges in knowledge mining from academic articles is reading the content of PDF files.","tags":["Artificial-intelligence","Knowledge","Pdf","Data-science","Machine-learning"],"title":"How to use GROBID","updated_at":1707773792,"url":"https://medium.com/@researchgraph/how-to-use-grobid-67df995b16fa"},{"abstract":null,"archive_url":null,"authors":[{"name":"Research Graph"}],"blog":{"api":false,"archive_prefix":null,"authors":null,"backlog":0,"canonical_url":null,"category":"computerAndInformationSciences","created_at":1706685423,"current_feed_url":null,"description":"Stories by Research Graph on Medium","favicon":"https://cdn-images-1.medium.com/fit/c/150/150/1*laJi0jBkVoGhXid7gD_DmQ.png","feed_format":"application/rss+xml","feed_url":"https://medium.com/@researchgraph/feed","filter":null,"funding":null,"generator":"Medium","generator_raw":"Medium","home_page_url":"https://medium.com/@researchgraph","id":"30da2ca9-8258-4ab5-acca-3919d9a5d98d","indexed":true,"issn":null,"language":"en","license":"https://creativecommons.org/licenses/by/4.0/legalcode","mastodon":"","plan":"Starter","prefix":"10.59350","relative_url":null,"ror":null,"secure":true,"slug":"researchgraph","status":"active","title":"Stories by Research Graph on Medium","updated_at":1708297648,"use_api":null,"use_mastodon":false,"user_id":"a7e16958-1175-437c-b839-d4b8a47ec811","version":"https://jsonfeed.org/version/1.1"},"blog_name":"Stories by Research Graph on Medium","blog_slug":"researchgraph","content_text":"<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1024/1*7Fy1vUav7gAiy9CAWanAvQ.png\" />\n<figcaption>Created using DALLE on 6 Feb\u00a02024</figcaption>\n</figure>\n\n#### Authors:\n\n- [Nakul Nambiar](https://www.linkedin.com/in/nakul-nambiar/)\n  ([0009--0009--9720--9233](https://orcid.org/0009-0009-9720-9233))\n- [Zhuochen Wu](https://www.linkedin.com/in/zhuochen-wu-45817a13b/)\n  ([0009--0000--5642--5348](https://orcid.org/0009-0000-5642-5348))\n\nResearch Graph is a structured representation of research objects that\ncaptures information about entities and the relationships between\nResearcher, Organisation, Publication, Grant and Research Data.\nCurrently, the publications are available as PDF files, and due to the\nfree-form text, it is difficult to parse a PDF file to extract\nstructured information. In this article, we will try to create a\nresearch graph from a PDF of a publication by extracting relevant\ninformation from the text and organising it into a graph structure\nusing\u00a0OpenAI.\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1024/1*40j-_agywNgUsJ_h_kXslA.jpeg\" />\n<figcaption>Pipeline to create the graph from\u00a0PDF</figcaption>\n</figure>\n\n### OpenAI\n\nIn this work, we use OpenAI API and the new Assistant functionality of\nGPT (Currently in Beta) to transform PDF documents to a set of\nstructured JSON files based on Research Graph\u00a0Schema.\n\n#### Assistants API\n\nThe Assistants API allows you to build AI assistants within your\napplications. An assistant can answer user questions by using models,\ntools, and information per predetermined guidelines. It is a beta API\nthat is actively under development. Using the assistants API, we can use\nthe OpenAI-hosted tools like code interpreter and knowledge retrieval.\nFor this blog post, we will focus on knowledge retrieval.\n\n#### Knowledge Retrieval\n\nSometimes, we need the AI model to answer queries based on unknown\nknowledge, like user-provided documents or sensitive information. We can\naugment the model with this information using the assistant APIs\nknowledge retrieval tool. We can upload files to the assistant, and it\nautomatically chunks the documents and creates and stores embeddings to\nimplement vector search on the\u00a0data.\n\n#### Example\n\nFor our example, we will upload PDF files of publications to the OpenAI\nassistant and the knowledge retrieval tool to get a JSON output of the\ngraph schema for the given publications. The publication we used for\nthis example can be accessed from the following\n[link](https://doi.org/10.5166/jroi-2-1-6).\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1024/1*CrAVHSNrRmQiBay81_Bi0Q.png\" />\n<figcaption>Screenshot from the publication (<a\nhref=\"https://doi.org/10.5166/jroi-2-1-6\">https://doi.org/10.5166/jroi-2-1-6</a>)</figcaption>\n</figure>\n\n#### Step 1\n\nRead the input path where the Publications PDFs are stored and the\noutput path where the JSON output will be\u00a0stored.\n\n    import configparser\n    config = configparser.ConfigParser()\n    config.read('{}/config.ini'.format(current_path))\n    input_path = config['DEFAULT']['Input-Path']\n    output_path = config['DEFAULT']['Output-Path']\n    debug = config['DEFAULT']['Debug']\n\n#### Step 2\n\nGet all the PDF files from the Input\u00a0path.\n\n    onlyfiles = [f for f in os.listdir(input_path) if os.path.isfile(os.path.join(input_path, f))]\n\n#### Step 3\n\nThen, we need to initialise the assistant to use the Knowledge retrieval\ntool. To do this, we need to specify the type of tools in the API for\n\"retrieval\". We also specify the instructions for the assistant and the\nOpenAI model to be\u00a0used.\n\n    my_file_ids = []\n    if client.files.list().data==[]:\n      for f in onlyfiles:\n        file = client.files.create(\n          file=open(input_path + f, \"rb\"),\n          purpose='assistants'\n        )\n        my_file_ids.append(file.id)\n\n    # Add the file to the assistant\n    assistant = client.beta.assistants.create(\n      instructions = \"You are a publication database support chatbot. Use pdf files uploaded to best respond to user queries in JSON.\",\n      model = \"gpt-4-1106-preview\",\n      tools = [{\"type\": \"retrieval\"}],\n      # Do not attach all files to the assistant, otherwise, it will mismatch the answers even though specify file ID in query messages.\n      # We will attach to each message instead\n    )\n\n#### Step 4\n\nThen, we specify the information we need to extract from the\npublications files and pass them as user queries to the assistant. After\nexperimenting with the assistant instructions, we found that requesting\nJSON format in each user message generates the most consistent output.\n\n    user_msgs = [\"Print the title of this paper in JSON\",\n                 \"Print the authors of this paper in JSON\",\n                 \"Print the abstract section of the paper in JSON\",\n                 \"Print the keywords of the paper in JSON\",\n                 \"Print the DOI number of the paper in JSON\",\n                 \"Print the author affiliations of the paper in JSON\",\n                 \"Print the reference section of the paper in JSON\"]\n\n#### Step 5\n\nThe next is to pass the queries to the assistant to generate the output.\nWe need to create an individual thread object for each user query, which\ncontains the query as a user message. Then, we run the thread and\nretrieve the assistant's answer.\n\n    all_results = []\n\n    for i in my_file_ids:\n        print('\\n#####')\n        # the JSON result it can extract and parse, hopefully\n        file_result = {}\n       \n        for q in user_msgs:\n            # create thread, user message and run objects for each query\n            thread = client.beta.threads.create()\n            msg = client.beta.threads.messages.create(\n                    thread_id=thread.id,\n                    role=\"user\",\n                    content=q,\n                    file_ids=[i] # specify the file/publication we want to extract from\n                )\n            print('\\n',q)\n            run = client.beta.threads.runs.create(\n                thread_id=thread.id,\n                assistant_id=assistant.id,\n                additional_instructions=\"If answer cannot be found, print 'False'\" # not very useful at the time of this presentation\n                )\n            # checking run status by retrieving updated object each time\n            while run.status in [\"queued\",'in_progress']:\n                print(run.status) \n                time.sleep(5)\n                run = client.beta.threads.runs.retrieve(\n                    thread_id=thread.id,\n                    run_id=run.id\n                    )\n            # usually a rate limit error\n            if run.status=='failed':  logging.info(\"Run failed: \", run)\n            if run.status=='completed':\n                print(\"<Complete>\")\n                # extract updated message object, this includes user messages\n                messages = client.beta.threads.messages.list(\n                    thread_id=thread.id\n                )\n                for m in messages:\n                    if m.role=='assistant':\n                        value = m.content[0].text.value # get the text response\n                        if \"json\" not in value:\n                            if value=='False':logging.info(\"No answer found for \", str(q))\n                            else:\n                                logging.info(\"Not JSON output, maybe no answer found in the file or model is outdated: \", str(value))\n                        else:\n                            # clean the response and try to parse as json\n                            value = value.split(\"```\")[1].split('json')[-1].strip()\n                            try: \n                                d = json.loads(value)\n                                file_result.update(d)\n                                print(d)\n                            except Exception as e:\n                                logging.info(f\"Query {q} \\nFailed to parse string to JSON: \", str(e))\n                                print(f\"Query {q} \\nFailed to parse string to JSON: \", str(e))\n                \n        all_results.append(file_result)\n\nThe JSON output generated for the above publication file\u00a0is:\n\n    [{'title': 'Dodes (diagnostic nodes) for Guideline Manipulation',\n      'authors': [{'name': 'PM Putora',\n        'affiliation': 'Department of Radiation-Oncology, Kantonsspital St. Gallen, St. Gallen, Switzerland'},\n       {'name': 'M Blattner',\n        'affiliation': 'Laboratory for Web Science, Z\u00fcrich, Switzerland'},\n       {'name': 'A Papachristofilou',\n        'affiliation': 'Department of Radiation Oncology, University Hospital Basel, Basel, Switzerland'},\n       {'name': 'F Mariotti',\n        'affiliation': 'Laboratory for Web Science, Z\u00fcrich, Switzerland'},\n       {'name': 'B Paoli',\n        'affiliation': 'Laboratory for Web Science, Z\u00fcrich, Switzerland'},\n       {'name': 'L Plasswilma',\n        'affiliation': 'Department of Radiation-Oncology, Kantonsspital St. Gallen, St. Gallen, Switzerland'}],\n      'Abstract': {'Background': 'Treatment recommendations (guidelines) are commonly represented in text form. Based on parameters (questions) recommendations are defined (answers).',\n       'Objectives': 'To improve handling, alternative forms of representation are required.',\n       'Methods': 'The concept of Dodes (diagnostic nodes) has been developed. Dodes contain answers and questions. Dodes are based on linked nodes and additionally contain descriptive information and recommendations. Dodes are organized hierarchically into Dode trees. Dode categories must be defined to prevent redundancy.',\n       'Results': 'A centralized and neutral Dode database can provide standardization which is a requirement for the comparison of recommendations. Centralized administration of Dode categories can provide information about diagnostic criteria (Dode categories) underutilized in existing recommendations (Dode trees).',\n       'Conclusions': 'Representing clinical recommendations in Dode trees improves their manageability handling and updateability.'},\n      'Keywords': ['dodes',\n       'ontology',\n       'semantic web',\n       'guidelines',\n       'recommendations',\n       'linked nodes'],\n      'DOI': '10.5166/jroi-2-1-6',\n      'references': [{'ref_number': '[1]',\n        'authors': 'Mohler J Bahnson RR Boston B et al.',\n        'title': 'NCCN clinical practice guidelines in oncology: prostate cancer.',\n        'source': 'J Natl Compr Canc Netw.',\n        'year': '2010 Feb',\n        'volume_issue_pages': '8(2):162-200'},\n       {'ref_number': '[2]',\n        'authors': 'Heidenreich A Aus G Bolla M et al.',\n        'title': 'EAU guidelines on prostate cancer.',\n        'source': 'Eur Urol.',\n        'year': '2008 Jan',\n        'volume_issue_pages': '53(1):68-80',\n        'notes': 'Epub 2007 Sep 19. Review.'},\n       {'ref_number': '[3]',\n        'authors': 'Fairchild A Barnes E Ghosh S et al.',\n        'title': 'International patterns of practice in palliative radiotherapy for painful bone metastases: evidence-based practice?',\n        'source': 'Int J Radiat Oncol Biol Phys.',\n        'year': '2009 Dec 1',\n        'volume_issue_pages': '75(5):1501-10',\n        'notes': 'Epub 2009 May 21.'},\n       {'ref_number': '[4]',\n        'authors': 'Lawrentschuk N Daljeet N Ma C et al.',\n        'title': \"Prostate-specific antigen test result interpretation when combined with risk factors for recommendation of biopsy: a survey of urologist's practice patterns.\",\n        'source': 'Int Urol Nephrol.',\n        'year': '2010 Jun 12',\n        'notes': 'Epub ahead of print'},\n       {'ref_number': '[5]',\n        'authors': 'Parmelli E Papini D Moja L et al.',\n        'title': 'Updating clinical recommendations for breast colorectal and lung cancer treatments: an opportunity to improve methodology and clinical relevance.',\n        'source': 'Ann Oncol.',\n        'year': '2010 Jul 19',\n        'notes': 'Epub ahead of print'},\n       {'ref_number': '[6]',\n        'authors': 'Ahn HS Lee HJ Hahn S et al.',\n        'title': 'Evaluation of the Seventh American Joint Committee on Cancer/International Union Against Cancer Classification of gastric adenocarcinoma in comparison with the sixth classification.',\n        'source': 'Cancer.',\n        'year': '2010 Aug 24',\n        'notes': 'Epub ahead of print'},\n       {'ref_number': '[7]',\n        'authors': 'Rami-Porta R Goldstraw P.',\n        'title': 'Strength and weakness of the new TNM classification for lung cancer.',\n        'source': 'Eur Respir J.',\n        'year': '2010 Aug',\n        'volume_issue_pages': '36(2):237-9'},\n       {'ref_number': '[8]',\n        'authors': 'Sinn HP Helmchen B Wittekind CH.',\n        'title': 'TNM classification of breast cancer: Changes and comments on the 7th edition.',\n        'source': 'Pathologe.',\n        'year': '2010 Aug 15',\n        'notes': 'Epub ahead of print'},\n       {'ref_number': '[9]',\n        'authors': 'Paleri V Mehanna H Wight RG.',\n        'title': \"TNM classification of malignant tumours 7th edition: what's new for head and neck?\",\n        'source': 'Clin Otolaryngol.',\n        'year': '2010 Aug',\n        'volume_issue_pages': '35(4):270-2'},\n       {'ref_number': '[10]',\n        'authors': 'Guarino N.',\n        'title': 'Formal Ontology and Information Systems',\n        'source': '1998 IOS Press'},\n       {'ref_number': '[11]',\n        'authors': 'Uschold M Gruniger M.',\n        'title': 'Ontologies: Principles Methods and Applications.',\n        'source': 'Knowledge Engineering Review',\n        'year': '1996',\n        'volume_issue_pages': '11(2)'},\n       {'ref_number': '[12]',\n        'authors': 'Aho A Garey M Ullman J.',\n        'title': 'The Transitive Reduction of a Directed Graph.',\n        'source': 'SIAM Journal on Computing',\n        'year': '1972',\n        'volume_issue_pages': '1(2): 131\u2013137'},\n       {'ref_number': '[13]',\n        'authors': 'Tai K',\n        'title': 'The tree-to-tree correction problem.',\n        'source': 'Journal of the Association for Computing Machinery (JACM)',\n        'year': '1979',\n        'volume_issue_pages': '26(3):422-433'}]}]\n\n#### Step 6\n\nFile objects and assistant object needs to be cleaned up because they\ncost money in 'retrieval' mode. Also, it is a good coding practice.\n\n    for f in client.files.list().data:\n        client.files.delete(f.id)\n\n    # Retrieve and delete running assistants\n    my_assistants = client.beta.assistants.list(\n        order=\"desc\",\n    )\n    for a in my_assistants.data:    \n        response = client.beta.assistants.delete(a.id)\n        print(response)\n\n#### Step 7\n\nThe next step is to generate a graph visualisation using the Python\n[Networkx](https://networkx.org/documentation/stable/tutorial.html)\u00a0package.\n\n    import networkx as nx\n    import matplotlib.pyplot as plt\n\n    G = nx.DiGraph()\n    node_colors = []\n\n    key = \"jroi/\" + all_results[0]['title']\n    G.add_nodes_from([(all_results[0]['title'], {'doi': all_results[0]['DOI'], 'title': all_results[0]['title'], 'source': 'jroi', 'key': key})])\n    node_colors.append('#4ba9dc')\n\n    for author in all_results[0]['authors']:\n        key = \"jroi/\" + author['name']\n        G.add_nodes_from([(author['name'], {'key': key, 'local_id': author['name'], 'full_name': author['name'], 'source': 'jroi'})])\n        G.add_edge(all_results[0]['title'], author['name'])\n        node_colors.append('#63cc9e')\n\n\n    for reference in all_results[0]['references']:\n        key = \"jroi/\" + reference['title']\n        G.add_nodes_from([(reference['title'].split('.')[0][:25] + '...', {'title': reference['title'], 'source': 'jroi', 'key': key})])\n        G.add_edge(all_results[0]['title'], reference['title'].split('.')[0][:25] + '...')\n        node_colors.append('#4ba9dc')\n\n    pos = nx.spring_layout(G)\n    labels = nx.get_edge_attributes(G, 'label')\n    nx.draw(G, pos, with_labels=True, node_size=1000, node_color=node_colors, font_size=7, font_color='black')\n    nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n    plt.savefig(\"graph_image.png\")\n    plt.show()\n\nThe graph visualisation is as\u00a0below:\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/640/1*-a5y19GUSyQZv63KMXKnZA.png\" />\n<figcaption>Graph Visualisation generated using\u00a0Networkx</figcaption>\n</figure>\n\n**Note:** Please note that the output structure generated by OpenAI may\nbe different for different executions. So you may need to update the\nabove code according to that structure.\n\n### Conclusion\n\nIn conclusion, leveraging the GPT API to extract a Research Graph from\nPDF publications offers a powerful and efficient solution for\nresearchers and data analysts. This workflow streamlines the process of\nconverting PDF publications into structured and accessible Research\nGraphs. But we must also pay careful attention to the inconsistencies of\nthe responses generated by LLM. Over time, accuracy and relevance may be\nfurther enhanced by routinely updating and improving the extraction\nmodel.\n\n![](https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=4655f64337db){width=\"1\"\nheight=\"1\"}\n","doi":"https://doi.org/10.59350/sqrk6-1bj22","guid":"https://medium.com/p/4655f64337db","id":"e21c6ce5-051e-4202-bea5-3111c1eda9a6","image":"https://cdn-images-1.medium.com/max/1024/1*7Fy1vUav7gAiy9CAWanAvQ.png","indexed_at":1,"language":"en","published_at":1707184406,"reference":[],"relationships":[],"summary":"<strong>\n Authors:\n</strong>\nNakul Nambiar (0009\u20130009\u20139720\u20139233)Zhuochen Wu (0009\u20130000\u20135642\u20135348) Research Graph is a structured representation of research objects that captures information about entities and the relationships between Researcher, Organisation, Publication, Grant and Research Data.","tags":["Artificalintelligence","Openai","Gpt","Data-science","Graph"],"title":"How to use GPT API to export a research graph from PDF publications","updated_at":1707184406,"url":"https://medium.com/@researchgraph/how-to-use-gpt-api-to-export-a-research-graph-from-pdf-publications-4655f64337db"},{"abstract":null,"archive_url":null,"authors":[{"name":"Research Graph"}],"blog":{"api":false,"archive_prefix":null,"authors":null,"backlog":0,"canonical_url":null,"category":"computerAndInformationSciences","created_at":1706685423,"current_feed_url":null,"description":"Stories by Research Graph on Medium","favicon":"https://cdn-images-1.medium.com/fit/c/150/150/1*laJi0jBkVoGhXid7gD_DmQ.png","feed_format":"application/rss+xml","feed_url":"https://medium.com/@researchgraph/feed","filter":null,"funding":null,"generator":"Medium","generator_raw":"Medium","home_page_url":"https://medium.com/@researchgraph","id":"30da2ca9-8258-4ab5-acca-3919d9a5d98d","indexed":true,"issn":null,"language":"en","license":"https://creativecommons.org/licenses/by/4.0/legalcode","mastodon":"","plan":"Starter","prefix":"10.59350","relative_url":null,"ror":null,"secure":true,"slug":"researchgraph","status":"active","title":"Stories by Research Graph on Medium","updated_at":1708297648,"use_api":null,"use_mastodon":false,"user_id":"a7e16958-1175-437c-b839-d4b8a47ec811","version":"https://jsonfeed.org/version/1.1"},"blog_name":"Research Graph","blog_slug":"researchgraph","content_text":"Authors: [Nakul Nambiar](https://www.linkedin.com/in/nakul-nambiar/),\n[Amir\u00a0Aryani](https://au.linkedin.com/in/amiraryani)\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1024/1*Gzmbpq5qJUz2W1zpHd1XGA.png\" />\n<figcaption>Created using DALL-E on 28 Jan\u00a02024</figcaption>\n</figure>\n\nKnowledge graphs, which offer a structured representation of data and\nits relationships, are revolutionising how we organise and access\ninformation. With large amounts of data, it sometimes becomes difficult\nto draw insights from it. This blog article examines how to combine\nNeo4j, a graph database, with OpenAI's Retrieval-Augmented Generation\n(RAG) model to build a robust knowledge management system.\n\nLLMs are pre-trained models and generate outputs based on the prompts we\npass them. Sometimes, the LLM results are precisely what you need, but\nother times, they just return random facts from their training data.\nRetrieval-augmented generation (RAG) is an AI framework that tries to\nimprove the response of the LLM models by providing context knowledge\nfrom external sources. Additional benefits of RAG\u00a0include:\n\n- Enables getting insights into private data without sharing or using it\n  for training.\n- Reduces the chances of 'Hallucination' and incorrect responses.\n- Reduces the need to train the model on new data continuously.\n\n### Sample Data\n\nFor our exercise, we will use OpenAI, combined with the Australian\nNational Graph Database, to create an RAG pipeline.\n\nThe [Australian National Graph\nDatabase](https://researchgraph.org/national-graph/) is a vast knowledge\ngraph with around 11 million nodes. We can't possibly fit all the data\ninto the OpenAI context. So, in the first phase, we will retrieve a\nsubset of the graph and then use this graph as a context for the OpenAI\nand to answer\u00a0prompts.\n\nWe use a Virtual Machine (VM) with a Neo4j server running the graph\ndatabase as a Neo4j server. The [graph\nschema](https://researchgraph.org/schema/) includes five primary node\ntypes that describe the research ecosystem: Researchers, Publications,\nDatasets, Grants and Organisations.\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1024/1*UocexTQHeuX3_VAwSJ03CQ.png\"\nalt=\"Research Graph Schema: https://researchgraph.org/schema/\" />\n<figcaption>Research Graph Schema. <a\nhref=\"https://researchgraph.org/schema/\">https://researchgraph.org/schema/</a></figcaption>\n</figure>\n\n### Step 1\n\nFirstly, we must establish the connection to the Neo4j database. Also,\nyou might notice that we use an OpenAI API key. This key can be\ngenerated at the OpenAI portal. A tutorial is available\n[here](https://elephas.app/blog/how-to-create-openai-api-keys-cl5c4f21d281431po7k8fgyol0).\n\n    import os\n    import json\n    import pandas as pd\n    import configparser\n    from neo4j import GraphDatabase\n\n    config = configparser.ConfigParser()\n    config.read('config.ini')\n\n    # Set the OpenAI API key env variable manually\n    os.environ[\"OPENAI_API_KEY\"] = config['DEFAULT']['openai-key']\n\n    # DB credentials\n    url = config['DEFAULT']['url']\n    username = config['DEFAULT']['username']\n    password = config['DEFAULT']['password']\n\n### Step 2\n\nWe get a subset of the graph from the larger dataset by querying Neo4j.\nThe output of the graph is retrieved in JSON\u00a0format.\n\n    def run_query(cypher_query):\n        with GraphDatabase.driver(url, auth=(username, password)) as driver:\n            with driver.session() as session:\n                result = session.run(cypher_query)\n                return result.data()\n\n    cypher_query = \"\"\"\n    MATCH (n:organisation)-[r]-(g:grant)\n    where g.source='arc.gov.au' RETURN * LIMIT 100\n    \"\"\"\n\n    result = run_query(cypher_query)\n\nNote: We have used a limit of 100 nodes in the response to limit the\nsize of the returning JSON file. This would impact the LLM functionality\nif it exceeds the context window\u00a0size.\n\nWe create a system prompt by providing background knowledge about the\ngraph database and the task. Along with it, we pass the JSON subgraph\nas\u00a0context.\n\n    system_prompt = f'''\n    You are a helpful agent designed to fetch information from a graph database.\n    Data\n    {json.dumps(result)}\n\n    This is the output of the Neo4j cypher which returns nodes and relationships. n is the organisations, g is the grants\n    and r is the relationships between the grants and organisations.\n\n    Based on this data answer the user prompts and return the output as json.\n\n    '''\n\nUsers can then enter the prompt to get insights from the subgraph. For\nexample, in the prompt below, we are trying to find the grant with the\nmost relationship to organisations.\n\n    prompt = \"Help me find the grants which is related to the most organisations\"\n\n### Step 3\n\nWe pass the system and user prompts to the OpenAI model. In this case,\nwe use \"gpt-4--1106-preview\". More information is available on the\n[OpenAI page on\u00a0Models](https://platform.openai.com/docs/models).\n\n    from openai import OpenAI\n    client = OpenAI()\n\n    def define_query(prompt, model=\"gpt-4-1106-preview\"):\n        completion = client.chat.completions.create(\n        model=model,\n        temperature=0,\n        response_format= {\n            \"type\": \"json_object\"\n        },\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": system_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n            ]\n        )\n        return completion.choices[0].message.content\n\nOpenAI provides the grant details with the most connections to\norganisations as a JSON\u00a0output.\n\n    print(define_query(prompt))\n\n### Output\n\nThis is the output provided by OpenAI\u00a0RAG.\n\n    {\n      \"grants\": [\n        {\n          \"arc_for_primary\": \"1117\",\n          \"local_id\": \"LP200301123\",\n          \"start_year\": \"2023\",\n          \"purl\": \"http://purl.org/au-research/grants/arc/LP200301123\",\n          \"source\": \"arc.gov.au\",\n          \"type\": \"grant\",\n          \"arc_scheme_name\": \"Linkage Projects\",\n          \"title\": \"Making Australia resilient to airborne infection transmission.\",\n          \"arc_announcement_administering_organisation\": \"Queensland University of Technology\",\n          \"url\": \"http://dataportal.arc.gov.au/NCGP/API/grants/LP200301123\",\n          \"arc_funding_at_announcement\": \"898013\",\n          \"arc_grant_status\": \"Active\",\n          \"arc_for\": \"040101,1117,111705,120404\",\n          \"r_number\": \"158880661\",\n          \"end_year\": \"2026\",\n          \"participant_list\": \"Lidia Morawska,Duzgun Agdas,Laurie Buys,Keith Grimwood,Graham Johnson,Guy Marks,Alexander Paz,Thomas Rainey,Zoran Ristovski,Kirsten Spann,Bo Xia,Patrick Chambers,Shaun Clough,Stephen Corbett,Charles dePuthod,Neal Durant,Travis Kline,Peter McGarry,Ross Mensforth,Clive Paige,John Penny,Brad Prezant,Uma Rajappa,David Ward,Scott Bell,Giorgio Buonanno,Mark Jermy,Jose Jimenez,Claire Wainwright,Sandra Glaister,greg Bell,Louisa O'Toole\",\n          \"key\": \"arc/LP200301123\",\n          \"related_organisations_count\": 24\n        }\n      ]\n    }\n\nWe can compare it visually with the graph from Neo4j and see that RAG\nprovided the correct\u00a0output.\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1024/1*PkIQl8fupR-TdiwppqdJkg.png\" />\n</figure>\n\nWe have tested this mini pipeline with other examples, and the\nsimplicity of the pipeline makes it functional as long as the size of\nthe subgraph produced in Step 2 does not exceed the context window of\nthe OpenAI\u00a0model.\n\n### Conclusion\n\nThe integration of OpenAI RAG and Neo4j paves the way for innovative\nsolutions in knowledge graph applications, and recommendation systems.\nRAG is efficient in grounding the LLM on the most recent data, and\nverified information and reduces the expense of retraining the model\nwhen new data is available.\n\n![](https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3a183df816ce){width=\"1\"\nheight=\"1\"}\n","doi":"https://doi.org/10.59350/4hcyx-g4j10","guid":"https://medium.com/p/3a183df816ce","id":"c6ec040f-3d25-47d8-b276-6897b5721c79","image":"https://cdn-images-1.medium.com/max/1024/1*Gzmbpq5qJUz2W1zpHd1XGA.png","indexed_at":1,"language":"en","published_at":1706151454,"reference":[],"relationships":[],"summary":"Authors: Nakul Nambiar, Amir\u00a0Aryani Knowledge graphs, which offer a structured representation of data and its relationships, are revolutionising how we organise and access information. With large amounts of data, it sometimes becomes difficult to draw insights from it. This blog article examines how to combine Neo4j, a graph database, with OpenAI\u2019s Retrieval-Augmented Generation (RAG) model to build a robust knowledge management system.","tags":["Retrieval Augmented","Graph Database","Knowledge Graph","Artificalintelligence","Neo4j"],"title":"Mini RAG using Neo4j","updated_at":1706151454,"url":"https://medium.com/@researchgraph/mini-rag-using-neo4j-3a183df816ce"},{"abstract":null,"archive_url":null,"authors":[{"name":"Research Graph"}],"blog":{"api":false,"archive_prefix":null,"authors":null,"backlog":0,"canonical_url":null,"category":"computerAndInformationSciences","created_at":1706685423,"current_feed_url":null,"description":"Stories by Research Graph on Medium","favicon":"https://cdn-images-1.medium.com/fit/c/150/150/1*laJi0jBkVoGhXid7gD_DmQ.png","feed_format":"application/rss+xml","feed_url":"https://medium.com/@researchgraph/feed","filter":null,"funding":null,"generator":"Medium","generator_raw":"Medium","home_page_url":"https://medium.com/@researchgraph","id":"30da2ca9-8258-4ab5-acca-3919d9a5d98d","indexed":true,"issn":null,"language":"en","license":"https://creativecommons.org/licenses/by/4.0/legalcode","mastodon":"","plan":"Starter","prefix":"10.59350","relative_url":null,"ror":null,"secure":true,"slug":"researchgraph","status":"active","title":"Stories by Research Graph on Medium","updated_at":1708297648,"use_api":null,"use_mastodon":false,"user_id":"a7e16958-1175-437c-b839-d4b8a47ec811","version":"https://jsonfeed.org/version/1.1"},"blog_name":"Research Graph","blog_slug":"researchgraph","content_text":"**Tools and Platform for Integration of Knowledge Graph with RAG\npipelines.**\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1024/1*bJ3eWZ7301vYDzBomwdLfQ.png\"\nalt=\"Complex network connected to books and showing information from magespace\" />\n<figcaption>Image Created in <a\nhref=\"https://www.mage.space/\">https://www.mage.space/</a></figcaption>\n</figure>\n\nAuthors: [Aland\nAstudillo](https://www.linkedin.com/in/aland-astudillo/), [Aishwarya\nNambissan](https://www.linkedin.com/in/aishwarya-nambissan-127229200/)\n\nMany users of chatbots such as ChatGPT, have encountered the problem of\nreceiving inappropriate or incompatible responses. There are several\nreasons why this might\u00a0happen.\n\nOne reason is the lack of appropriate training data, as chatbots are\nusually trained on large amounts of text and code. If the data is\ninsufficient or of poor quality, the chatbot may misunderstand queries\nand provide inaccurate responses. Another reason is that some chatbots\nare designed for specific tasks or domains, which limits their ability\nto handle broader queries or understand subtle nuances in conversation.\nAdditionally, chatbots may struggle with natural language, which is\ncomplex and often ambiguous. This can cause them to misunderstand a\nuser's query and provide irrelevant or off-topic responses. Finally,\nthere are technical limitations, such as the chatbot's inability to\nreason or make inferences.\n\nThis article explores a potential solution by combining two influential\napproaches in the field of Natural Language Processing\u200a---\u200aRetrieval\nAugmented Generation (**RAG**) and Knowledge Graphs(**KGs**). We will\ndelve into the partnership between these two entities, discuss the\nnotable technologies and software used in their processes, and highlight\nvarious options for utilizing their combined potential.\n\n### **RAG**\n\nRetrieval-Augmented Generation is the process of optimizing the output\nof a large language model using a knowledge base outside of its training\ndata sources before generating a response. It takes an input and\nretrieves a set of relevant/supporting documents given a source (e.g.,\nWikipedia). This can be thought of as a Large Language Model (LLM) not\njust putting words together, but carefully selecting relevant\ninformation from external sources and Knowledge Graphs to create\nwell-informed and detailed responses.\n\n### RAG Retrieval Techniques\n\nThe following are some crucial technologies that enable RAG's impressive\nability to retrieve and incorporate relevant information:\n\n**Vector Search**: It transforms text into numerical vectors, capturing\ntheir meaning and nuances in a mathematical space, creating a map of\nrelationships. Similar texts, like those discussing shared topics or\nusing similar language, end up positioned close together in this space,\nallowing vector search to quickly identify them as related. This allows\nlightning-fast comparisons, finding similar texts based on meaning, not\njust keywords.\n\nAlgorithms like [**Faiss**](https://github.com/facebookresearch/faiss)\nand [**Annoy**](https://github.com/spotify/annoy) map text into dense\nvectors, enabling fast comparisons and retrieval of relevant passages\nbased on semantic similarity.\n\n**Passage Ranking**: It is an internal algorithm that scores candidate\ntext passages based on their relevance to a query. It considers factors\nlike keyword frequency, keyword overlap, and document structure to act\nlike a judge, sifting through information to select the most fitting and\ninformative passages.\n\nKeyword overlap measures how often the same keywords appear in **both**\nthe query and the candidate passage, emphasizing shared vocabulary and\npotential relevance. It differs from keyword frequency, which simply\ncounts how often individual keywords appear within a passage, regardless\nof their presence in the\u00a0query.\n\nTechniques like [**BM25**](https://github.com/getalp/wikIR) and\n[**TF-IDF**](https://github.com/marcocor/wikipedia-idf) score candidate\npassages based on keyword overlap and frequency, ensuring retrieved\ninformation truly fits the\u00a0context.\n\n**Graph Neural Networks** (**GNNs**): They are neural networks designed\nto explore and learn from interconnected data like maps, social\nnetworks, and other complex relationships. Unlike traditional processing\nmethods that go through data in a linear fashion, GNNs are capable of\nrecognizing hidden patterns and understanding relationships like \"who\nknows who\" and \"what connects to what\" by \"hopping\" across connections\nin\u00a0data.\n\nConsider a graph as a network of dots(nodes) connected by lines (edges).\nEach dot represents some information, like a person, object, or concept.\nThe lines tell you how these things relate to each\u00a0other.\n\nGNNs work in rounds. In each\u00a0round:\n\n1.  Message Passing: Each node \"talks\" to its neighbors, sending\n    messages along the edges. These messages contain information about\n    the node itself and its features.\n2.  Node Update: Each node receives messages from all its neighbors and\n    combines them with its own information. This update can involve\n    calculations and applying a special function.\n3.  Output Calculation: Based on the updated information, the network\n    calculates an output for each node. This output could be a\n    prediction about the node's category, its relationship to another\n    node, or some other relevant information.\n\nThis process repeats for multiple rounds, allowing nodes to incorporate\ninformation from their entire neighborhood, not just their direct\nneighbors. As the rounds progress, the network learns to understand the\nrelationships between nodes and the overall structure of the\u00a0graph.\n\nWhen dealing with Knowledge Graphs, frameworks like\n[**PyTorch-Geometric**](https://readthedocs.org/projects/pytorch-geometric/)\nand [**DeepMind's\nGNN**](https://github.com/deepmind/deepmind-research/blob/master/learning_to_simulate/graph_network.py)\nlibrary come into play. These frameworks allow GNNs to traverse\ninterconnected entities and relationships within the graph, retrieve\nrelevant knowledge fragments, and understand complex connections.\n\n### **Knowledge Graphs: The Structured Wisdom\u00a0Library**\n\nA knowledge graph, also referred to as a semantic network, is a\nstructure that represents a network of real-world entities such as\nobjects, events, situations, or concepts. It helps to illustrate the\nconstantly changing representations of the world, connecting entities\n(such as \"Marie Curie\") and relationships (such as \"won Nobel Prize\") to\nform a complex network of information. This information is typically\nstored in a graph database and visualized as a graph structure, thus the\nterm knowledge \"graph\".\n\nKGs go beyond simply finding relevant facts and delve deeper into\nunderstanding the relationships and insights hidden within using these\nprocesses:\n\n**Entity Linking**: Imagine a vast network of information, like a big\npuzzle of dots. Now imagine trying to connect specific names, places,\nand concepts to their corresponding dots in the puzzle. That is what\nentity linking does with text and knowledge graphs, connecting the\nspecific components of the text to the corresponding nodes in the graph.\nThey help systems understand the exact meaning of entities, and find\nrelevant information from the\u00a0graph.\n\nLibraries like [**DGL-KeLP**](https://github.com/awslabs/dgl-ke)\nleverage GNNs to identify and link named entities (like \"Marie Curie\")\nto their respective nodes within the Knowledge Graphs, enabling RAG to\nretrieve information that is directly relevant to the core subject of a\nsearch\u00a0query\n\n**Path Mining**: Path mining is a process of uncovering hidden\nrelationships and patterns that are not easily noticeable. It involves\nexploring complicated networks of information and identifying and\ntracing connections between entities that may seem unrelated. By doing\nso, path mining reveals surprising insights and useful knowledge,\nimproving our understanding of the complex structures within knowledge\ngraphs.\n\nTools like [**Neo4j**](https://neo4j.com/) and\n[**Stanza**](https://github.com/stanfordnlp/stanza) allow traversing\npaths between entities, uncovering hidden relationships, and generating\ninsightful responses based on this deeper understanding.\n\n**Reasoning and Inference**: In the context of knowledge graphs,\nreasoning and inference are not just limited to discovering facts; they\nare also concerned with utilizing them effectively. This involves\nintegrating data, drawing meaningful connections, and using logical\nreasoning to resolve issues, foresee future occurrences, or even\nconstruct narratives leveraging the insights provided by the knowledge\ngraph.\n\nConsider the scenario of trying to find an organization that works in\nspecific sectors with the help of a knowledge graph. This analogy\neffectively highlights the active role of reasoning and inference in\nknowledge graphs:\n\n1.  Gathering Facts: Knowledge graphs collect and organize information\n    from various sources, such as websites, databases, academic papers,\n    and social media platforms. These facts are represented as\n    structured data, with entities (e.g., organizations) and their\n    attributes (e.g., sectors in which they operate) forming nodes and\n    edges in the graph. By combining data about organizations and\n    sectors, knowledge graphs enable the gathering of relevant facts for\n    analysis.\n2.  Integrating information: By connecting an organization's\n    relationships with specific sectors, such as partnerships,\n    investments, or certifications, knowledge graphs reveal the scope\n    and relevance of their work within those sectors. Links to related\n    entities like employees, board members, or projects can further\n    contribute to understanding an organization's involvement in\n    specific\u00a0sectors.\n3.  Predicting and Creating: Knowledge graphs can leverage machine\n    learning and predictive models to infer missing or hidden\n    information. By analyzing the available facts and connections within\n    the graph, these models can predict an organization's potential\n    involvement in sectors that have common attributes with their known\n    areas of operation. For example, if an organization has expertise in\n    renewable energy, predictive models could suggest their likely\n    involvement in related sectors like clean transportation or\n    sustainable infrastructure. Additionally, knowledge graphs\n    facilitate the creation of new information and insights by combining\n    existing facts with external data sources. For instance, by\n    integrating real-time data on industry trends, market analysis, or\n    news articles, knowledge graphs enable the discovery of emerging\n    sectors or upcoming organizations that might align with the given\n    parameters.\n\nA framework like [**Atomspace**](https://github.com/opencog/atomspace)\nfrom [**OpenCog**](https://opencog.org/) empowers RAG to reason and\ninfer new knowledge. By traversing paths and combining information from\ninterconnected entities, the system can generate informed predictions or\nanswer hypothetical questions.\n\n### Purpose\n\nThe combination of Retrieval-Augmented Generation (RAG) and Knowledge\nGraphs (KG) is beneficial for several\u00a0reasons:\n\n1.  **Enhanced information retrieval**: Knowledge graphs provide\n    structured and interconnected information that can significantly\n    improve the effectiveness of information retrieval. By using KGs,\n    RAG models can retrieve more accurate and relevant information,\n    leading to better generation and response\u00a0quality.\n2.  **Reliable and diverse information:** KGs are constructed from\n    authoritative sources, making them reliable and trustworthy sources\n    of information. RAG models can leverage this reliable information to\n    generate more accurate responses. Additionally, KGs help in\n    diversifying the generated responses by providing a broader pool of\n    related facts and entities.\n3.  **Context-aware understanding**: KGs enable RAG models to understand\n    and reason over the contextual information. By leveraging the\n    relationships and semantic connections encoded in KGs, RAG models\n    can better grasp the context of user queries or conversations,\n    resulting in more coherent and appropriate responses.\n4.  **Handling complex queries**: KGs allow RAG models to tackle complex\n    queries by breaking them down into smaller sub-queries, retrieving\n    relevant pieces of information from the KG, and then generating a\n    response based on the retrieved knowledge. This enables RAG models\n    to handle a wide range of user queries effectively.\n5.  **Explainability and transparency**: KGs provide a transparent and\n    interpretable representation of knowledge. By integrating KG-based\n    retrieval into RAG models, the reasoning behind the generated\n    responses becomes more explainable. Users can have a clear\n    understanding of the knowledge sources and connections used to\n    produce the response.\n6.  **Scalability**: Knowledge graphs act as large-scale repositories of\n    information. RAG models can leverage KGs to generate responses to\n    various queries or conversations without requiring additional\n    supervised training data. This makes the RAG+KG approach scalable to\n    handle an extensive range of knowledge domains and user\u00a0queries.\n\n### **Pipeline Possibilities: Orchestrating RAG and\u00a0KGs:**\n\nLet's explore some exciting pipeline options for harnessing the combined\npower of RAG and Knowledge Graphs. There are two options in which either\nthe LLM is prioritized or the Knowledge Graph is prioritized:\n\n**Option 1: LLM-Centric Pipeline:**\n\nThe LLM-Centric pipeline is a RAG and Knowledge Graph combination that\nempowers LLMs to craft well-informed responses. Here's how it\u00a0works:\n\n1.  Start with the user's question or statement\n2.  The LLM (like GPT-3) generates an initial draft response based on\n    its internal knowledge. This draft may lack specific factual details\n    or nuances that a knowledge graph can\u00a0provide.\n3.  RAG kicks in, searching the text corpus or the Knowledge Graph for\n    relevant passages that enrich the draft. During the retrieval\n    process, RAG retrieval techniques are used to search not only text\n    corpora but also knowledge graphs to find relevant information. This\n    means that RAG can directly tap into the structured knowledge within\n    the graph to retrieve facts, relationships, and entities that align\n    with the user's query and the LLM's generated draft.\n4.  The retrieved information is carefully fused with the LLM's output,\n    creating a more factually accurate and insightful response\n5.  A final polishing step ensures the response is fluent, grammatically\n    correct, and ready to\u00a0show.\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1024/0*3pd9MOIflkbS07wI\" />\n<figcaption>RAG LLM-centric generic\u00a0scheme.</figcaption>\n</figure>\n\nThe basic steps to perform this\u00a0are:\n\n1.  **Pre-processing**: Clean and tokenize user input to prepare for\n    processing.\n2.  **LLM Generation**: Generate an initial draft response using an LLM\n    like [**GPT-3**](https://openai.com/product) or [**Jurassic-1\n    Jumbo**](https://www.livescience.com/google-sentient-ai-lamda-lemoine).\n3.  **Retrieval**: Employ RAG techniques to retrieve relevant passages\n    from a text corpus or Knowledge Graphs.\n4.  **Fusion**: Integrate retrieved information into the LLM-generated\n    draft, creating a more informed and factually-grounded response.\n5.  **Post-processing**: Refine the final response for fluency,\n    grammatical correctness, and overall coherence.\n\n**Option 2: Knowledge Graphs-Centric Pipeline:**\n\nIn this approach, knowledge graphs take center stage. In essence, this\npipeline prioritizes the structured knowledge within knowledge graphs,\nusing RAG retrieval techniques to translate those insights into\ncompelling and informative language. Here's how it\u00a0unfolds:\n\n1.  User input: The process begins with the user's question or statement\n2.  Graph exploration: The knowledge graph is meticulously explored to\n    identify relevant entities, relationships, and paths that align with\n    the user's input. This stage involves techniques like entity\n    linking, path mining, and reasoning to uncover valuable information\n    within the\u00a0graph\n3.  Response planning: The insights extracted from the graph are used to\n    create a structured response plan. This plan outlines the key\n    points, facts, and logical flow that the final response\n    should\u00a0embody\n4.  Language generation: This is where RAG steps in. Its purpose is to\n    create human-like text that follows the response plan. It uses LLMs\n    to produce well-written sentences and paragraphs, combining the\n    relevant information from the knowledge graph while maintaining\n    cohesiveness and readability.\n5.  Post-processing: The generated response undergoes a final refinement\n    process to ensure grammatical correctness, clarity, and\n    overall\u00a0quality\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1024/0*mZ83esKBjbPmCq_C\" />\n<figcaption>RAG Knowledge Graph-centric generic\u00a0scheme.</figcaption>\n</figure>\n\nThe basic steps\u00a0are:\n\n1.  **Query Formulation**: Transform the user input into a query\n    suitable for Knowledge Graph's exploration.\n2.  **Knowledge Graphs:** You can use either Neo4j or\n    [NebulaGraph](https://www.nebula-graph.io/) to implement a retrieval\n    enhancement technique. This technique involves utilizing a knowledge\n    graph to illustrate the connections between entities and\n    relationships. Additionally, it incorporates a powerful language\n    model to improve the retrieval process.\n3.  **Fact Selection**: Employ entity linking and reasoning algorithms\n    to select and prioritize the most relevant facts based on the query\n    and\u00a0context.\n4.  **Natural Language Generation** (**NLG**): Utilise specialized NLG\n    models like\n    [BART](https://research.facebook.com/publications/controllable-abstractive-summarization/)\n    to translate the extracted facts into a natural language response.\n5.  **Refinement**: Enhance the generated response for clarity and\n    coherence.\n\n### **Unveiling a Future of Intelligent Interaction**\n\nThe combination of RAG and Knowledge Graphs goes beyond just being a\ntechnological fusion. It paves the way for a future where the\ninteraction between humans and computers goes beyond simple words and\nbecomes a more informed and refined form of communication. As these\ntechnologies continue to develop, we can expect to witness a significant\ntransformation in:\n\n- AI-powered assistants that answer your questions with the confidence\n  of a well-read friend, seamlessly combining relevant facts and\n  insights gleaned from Knowledge Graphs.\n- Next-generation search engines that go beyond keyword matching,\n  understanding the deeper meaning behind your queries and delivering\n  comprehensive, contextual results enriched with information from\n  Knowledge Graphs.\n- Creative writing tools that utilize RAG and Knowledge Graphs to\n  generate stories that are both factually accurate and full of\n  unexpected plot twists and character development, moving beyond\n  clich\u00e9d patterns.\n\n### **Conclusion**\n\nThe convergence of Retrieval Augmented Generation (RAG) and Knowledge\nGraphs (KGs) brings about an exciting synergy in the world of Natural\nLanguage Processing (NLP). RAG enhances the output of large language\nmodels by carefully selecting relevant information from external sources\nand KGs, allowing for well-informed and detailed responses. KGs, on the\nother hand, provide a structured representation of real-world entities\nand their relationships, enabling the exploration of hidden insights and\nthe discovery of complex connections.\n\nThe integration of RAG and KGs opens up two pipeline possibilities. The\nLLM-centric pipeline prioritizes the language model's output, which is\nthen enriched with information retrieved from KGs. The Knowledge\nGraphs-centric pipeline, on the other hand, places KGs at the center,\nutilizing RAG techniques to translate the structured insights into\ncompelling and informative language.\n\nWhile integrating LLMs and a knowledge graph for content retrieval\nrequires careful planning, the reward is significant. You can gain\naccess to hidden relationships within information, ultimately leading to\nhigher-quality output information.\n\nTools like **OpenAI**, **Langchain**, and **LlamaIndex** provide\nready-made pipelines to integrate knowledge graphs (like **Neo4j**)\neasily. Meanwhile, open-source LLMs like **Mistral**, **Llama**, and\n**Dolphin** are catching up to proprietary models in performance, making\nthem attractive choices for building custom architectures. This\nopen-source scenario allows for the exploration and examination of\nvarious methods before fully committing to a particular technological\nframework. So, it is crucial to evaluate your needs and choose the\napproach that best fits your use\u00a0case.\n\n![](https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fc0a6900f7eb){width=\"1\"\nheight=\"1\"}\n","doi":"https://doi.org/10.59350/jhrs4-22440","guid":"https://medium.com/p/fc0a6900f7eb","id":"05f01f68-ef81-47d7-a3c1-40aba91d358f","image":"https://cdn-images-1.medium.com/max/1024/1*bJ3eWZ7301vYDzBomwdLfQ.png","indexed_at":1,"language":"en","published_at":1705557796,"reference":[],"relationships":[],"summary":"<strong>\n Tools and Platform for Integration of Knowledge Graph with RAG pipelines.\n</strong>\nAuthors: Aland Astudillo, Aishwarya Nambissan Many users of chatbots such as ChatGPT, have encountered the problem of receiving inappropriate or incompatible responses. There are several reasons why this might\u00a0happen. One reason is the lack of appropriate training data, as chatbots are usually trained on large amounts of text and code.","tags":["Artificial Intelligence","Machine Learning","Retrieval Augmented","Knowledge Graph"],"title":"Unveiling the Synergy: Retrieval Augmented Generation (RAG) Meets Knowledge Graphs","updated_at":1705557796,"url":"https://medium.com/@researchgraph/unveiling-the-synergy-retrieval-augmented-generation-rag-meets-knowledge-graphs-fc0a6900f7eb"},{"abstract":null,"archive_url":null,"authors":[{"name":"Research Graph"}],"blog":{"api":false,"archive_prefix":null,"authors":null,"backlog":0,"canonical_url":null,"category":"computerAndInformationSciences","created_at":1706685423,"current_feed_url":null,"description":"Stories by Research Graph on Medium","favicon":"https://cdn-images-1.medium.com/fit/c/150/150/1*laJi0jBkVoGhXid7gD_DmQ.png","feed_format":"application/rss+xml","feed_url":"https://medium.com/@researchgraph/feed","filter":null,"funding":null,"generator":"Medium","generator_raw":"Medium","home_page_url":"https://medium.com/@researchgraph","id":"30da2ca9-8258-4ab5-acca-3919d9a5d98d","indexed":true,"issn":null,"language":"en","license":"https://creativecommons.org/licenses/by/4.0/legalcode","mastodon":"","plan":"Starter","prefix":"10.59350","relative_url":null,"ror":null,"secure":true,"slug":"researchgraph","status":"active","title":"Stories by Research Graph on Medium","updated_at":1708297648,"use_api":null,"use_mastodon":false,"user_id":"a7e16958-1175-437c-b839-d4b8a47ec811","version":"https://jsonfeed.org/version/1.1"},"blog_name":"Research Graph","blog_slug":"researchgraph","content_text":"A novel approach to improving the efficiency of text search in graph\ndatabases utilizing Neo4j, OpenAI, and Typesense.\n\nAuthors: [Nakul Nambiar](https://www.linkedin.com/in/nakul-nambiar/),\n[Aishwarya\nNambissan](https://www.linkedin.com/in/aishwarya-nambissan-127229200/)\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1024/1*3_mdx5SSgImdy1c2RZzl1A.png\" />\n<figcaption>Created using\u00a0DALL-E</figcaption>\n</figure>\n\nThe ability to use cutting-edge tools and frameworks is essential for\nstaying ahead in the ever-changing field of technology. When it comes to\nnatural language processing, search capabilities, and graph databases,\n**LLM** (Large Language Models), **Typesense**, and **Neo4j** work\ntogether as a powerful trio that offers a cohesive and effective\nsolution.\n\nAs we explore further, we will learn about the unique advantages of\nTypesense, Neo4j, and LLM and how their combined powers may open up a\nnew world of possibilities. This blog is your resource for\ncomprehending, putting into practice, and optimizing the potential of\nthis influential trio, whether you're a developer looking for creative\nsolutions, a data scientist seeking improvements in analytics, or a\nbusiness professional trying to maximize information retrieval.\n\n### What is Neo4j and Typesense?\n\nNeo4j is a well-known graph database that specializes in managing\ndensely linked data. Because of its superior relationship management\ncapabilities, it is a well-known option for applications with complex\ndata models. Conversely, Typesense is an extremely quick, open-source\nsearch engine that can quickly search through and retrieve a lot of\ntext-based data.\n\nWhile Neo4j is a powerful tool for traversing relationships and querying\ngraph data, its built-in text search features are not effective for\nhandling large amounts of text. Here, we will use Typesense to enhance\nNeo4j's text search capabilities by using a customized solution for\neffective and quick search operations.\n\n### Text Search Challenges in\u00a0Neo4j\n\n**Memory**: Neo4j relies heavily on memory. Thus, it may consume a lot\nof memory for intensive text search\u00a0queries.\n\n**Lack of Specialised Text Search Features**: Neo4j is a graph database,\nand its main function is to handle and store graph-based data. As\ncompared to other search tools, it lacks some specialized text search\nfeatures, which may have a negative impact on the\u00a0result.\n\nIn this article, we are using a Neo4j Graph database that uses this\n[schema](https://researchgraph.org/schema) to demonstrate the challenges\nfor text search and potentially a working solution. The graph includes\nfive primary objects including research publications, researchers,\norganizations, academic grants, and research data collections, capturing\nthe collaborative network of academic\u00a0works.\n\nThe following is a subset of the schema that contains information\nrelevant to this\u00a0article.\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1024/1*6l7kfZAin3Bwp_gZdU6U1g.jpeg\" />\n<figcaption>Publication and grant schema from <a\nhref=\"https://researchgraph.org/schema/\">Research Graph\nMeta\u00a0Model</a></figcaption>\n</figure>\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1024/1*lrtN5375h6OAuwtGN9KNYA.jpeg\" />\n<figcaption>Example of the Research Graph\u00a0schema</figcaption>\n</figure>\n\nTo illustrate the difficulty at hand, if we wish to find research grants\nand publications associated with 'Artificial Intelligence', we will\nencounter the following issues:\n\n\\(a\\) We will need to find all alternative or relevant terms (e.g. AI,\nML,\u00a0\\...)\n\n\\(b\\) We will need to have a (computationally) expensive text index\nsearch across Neo4j nodes as Neo4j does not particularly perform well\nfor text\u00a0search.\n\nTo address this issue we are employing a text-based search engine called\nTypesense. But first, let\\'s look at the advantages and capabilities of\nboth tools in our\u00a0toolbox.\n\n### Advantages of\u00a0Neo4j\n\n**Flexibility:** The graph model is adaptable and may quickly change to\nreflect new relationships and data structures.\n\n**Cypher Query Language:** Neo4j uses cypher query language, which is\nexpressive, readable, and more suitable for graph databases. It makes\nretrieving complex patterns from graph databases easier.\n\n**Performance:** Neo4j's underlying architecture is optimized to query\nnodes and relationships in a graph efficiently, thus improving the\nperformance.\n\n**Scalability:** Neo4j supports horizontal scalability. It lets you add\nmore machines to handle large datasets when\u00a0needed.\n\n**Transactional Support:** Neo4j ensures data consistency and integrity\nin transactional operations by adhering to the ACID (Atomicity,\nConsistency, Isolation, Durability) standards.\n\n### Advantages of Typesense\n\n**Speed**: As a search engine designed just for text queries, Typesense\nperforms very well in terms of speed. We can offload the text-search\ntasks to Typesense and increase the speed of the application.\n\n**Rich Text Search**: Typesense provides various advanced text search\nfeatures, typo tolerance, faceted, hybrid, and fuzzy search. Hence, it\ncan accommodate typos and potential errors in user\u00a0inputs.\n\n**Scalable**: Typesense can scale horizontally. Thus, it can handle\nlarge datasets and a huge volume of\u00a0queries.\n\n**Easy Integration**: Typesense has simple APIs and connectors for\ndifferent programming languages, making integrating with other platforms\nand software accessible.\n\n### Vector Search\n\nTypesense can use various small LLM models to create embeddings for the\ntext and then use these embeddings to do a nearest neighbor\nsearch\u00a0(KNN).\n\nAn embedding for a document is an alternate representation of data as\nfloating point values. These embeddings are generated using Machine\nLearning models. In principle, two similar documents would have\nembeddings \"closer\" to each\u00a0other.\n\nFor example, using OpenAI, we can convert text to embeddings using\nOpenAI APIs as\u00a0below:\n\n    from openai import OpenAI\n    client = OpenAI()\n\n    response = client.embeddings.create(\n        input=\"Your text string goes here\",\n        model=\"text-embedding-ada-002\"\n    )\n\n    print(response.data[0].embedding)\n\nThe generated embeddings will be as\u00a0follows:\n\n    {\n      \"data\": [\n        {\n          \"embedding\": [\n            -0.006929283495992422,\n            -0.005336422007530928,\n            ...\n            -4.547132266452536e-05,\n            -0.024047505110502243\n          ],\n          \"index\": 0,\n          \"object\": \"embedding\"\n        }\n      ],\n      \"model\": \"text-embedding-ada-002\",\n      \"object\": \"list\",\n      \"usage\": {\n        \"prompt_tokens\": 5,\n        \"total_tokens\": 5\n      }\n    }\n     \n\nTypesense converts text in each document using the LLM model mentioned\nin the\u00a0schema.\n\nHere are some common models you can use to generate these document\nembeddings:\n\n- [Sentence-BERT](https://www.sbert.net/)\n- [E-5](https://medium.com/@hansahettiarachchi/unleashing-the-potential-of-embedding-model-e5-revolutionizing-natural-language-comprehension-3f1516489048)\n- [CLIP](https://github.com/openai/CLIP)\n- [OpenAI's Text Embeddings\n  model](https://platform.openai.com/docs/api-reference/embeddings/create)\n- [Google's\n  PaLM\u00a0API](https://developers.generativeai.google/products/makersuite)\n- [Google's\n  Vertex\u00a0API](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings)\n\nYou can find the list of built-in models supported by Typesense\n[here](https://huggingface.co/typesense/models/tree/main).\n\nFor example, below is a schema for a collection of grants that creates\nembeddings for the attribute \"grant-summary\" using the\n\"all-MiniLM-L12-v2\" LLM model for creating the embeddings. The created\nembeddings are stored as a new attribute called embedding in the same\ndocument.\n\n    grant_schema = {\n      \"name\": \"grant\",\n      \"fields\": [\n        {\"name\": \"grant-summary\", \"type\": \"string\"},\n        {\"name\": \"flat-field-of-research\", \"type\": \"string\"},\n        {\n                \"name\": \"embedding\",\n                \"type\": \"float[]\",\n                \"embed\": {\n                  \"from\": [\n                    \"grant-summary\"\n                  ],\n                  \"model_config\": {\n                    \"model_name\": \"ts/all-MiniLM-L12-v2\"\n                  }\n                }\n              }\n        ]\n    }\n\n### Demo\n\nWe will make use of OpenAI, Typesense, and Neo4j to get relevant data\nfor a keyword from the Neo4j graph database.\n\n**Resources**: We have a Virtual Machine (VM) with a Neo4j server\nrunning with the grants data embedded on the \"grant-summary\" attribute.\nTypesense is running on the same VM on port 8108. Lastly, we need an\nOpenAI API key that can be generated from\u00a0[OpenAI](https://openai.com/).\n\nIn this example, we will search for a keyword and all related related\nterms. We will use OpenAI to generate the related words to the keyword.\nWe will then use these related words to do a semantic search in the\nTypesense database to get a list of IDs. Using this list of IDs, we can\nthen query the Neo4j database to get the\u00a0graph.\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1024/1*5ng10X_cLfH_1o2rvK-cwQ.jpeg\" />\n<figcaption>Pipeline steps for OpenAI-Typesense-Neo4j integrated\nsearch</figcaption>\n</figure>\n\n1.  First, we accept the user input, which is the keyword that needs to\n    be searched.\n\n2\\. Then, this keyword is passed to OpenAI, which generates a set of 5\nkeywords that are related to the given\u00a0keyword.\n\n    prompt = \"We are searching academic articles for research related to a\n              keyword presented by the user. Recommend five alternative terms \n              that find related papers to the given keyword. Only create keywords \n              and no other description in your response. Produce CSV\"\n\n    user_prompt = \"The keyword I want search for articles is\" + search_query_term\n\n\n    import openai\n\n    completion = openai.ChatCompletion.create(\n      model=\"gpt-4-1106-preview\",\n      messages=[ \n        {\"role\": \"system\", \"content\": prompt},\n        {\"role\": \"user\", \"content\": user_prompt}\n      ]\n    )\n\nFor the keyword \"Artificial Intelligence\", OpenAI generated the\nfollowing output\u00a0below:\n\n    {\n     \"role\": \"assistant\",\n     \"content\": \"\\\"machine learning\\\", \\\"neural networks\\\", \\\"cognitive computing\\\", \\\"natural language processing\\\", \\\"robotics\\\"\"\n    }\n\n3\\. The generated keywords are then searched in Typesense to get a set\nof DOIs related to the keywords.\n\n    import typesense\n\n    client = typesense.Client({\n      'nodes': [{\n        'host': 'localhost', \n        'port': '8108',      \n        'protocol': 'http'\n      }],\n      'api_key': api_key,\n      'connection_timeout_seconds': 300\n    })\n\n    search_parameters = {\n        'searches': [],\n        \n    }\n\n    for term in completion.choices[0].message[\"content\"].split(','):\n        search_object = {\n          'q': term.strip(),\n          'query_by': 'embedding',\n          'exclude_fields': 'embedding',\n          'limit': 2,\n          'collection': 'publications'\n        }\n        search_parameters[\"searches\"].append(search_object)\n\n    common_search_params={}\n    data = client.multi_search.perform(search_parameters, common_search_params);\n    dois = []\n    for search_result in data['results']:\n        for doc in search_result['hits']:\n            dois.append(doc['document']['DOI'])\n            print(doc['document']['DOI'] , ' ' , doc['document']['title'])\n\n4\\. DOIs can then be easily queried from the Neo4j database to get the\nrelated data and\u00a0graph.\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1024/1*27ltmtCCpQPnPvkj6uuetw.png\" />\n<figcaption>Graph for the keyword \u201cArtificial Intelligence\u201d</figcaption>\n</figure>\n\n### Conclusion\n\nTo sum up, the integration of Neo4j, OpenAI, and Typesense is a\ncombination that tackles the difficulties related to text search in\ngraph databases.\n\nWith this integration, developers can take advantage of the benefits of\neach of the platforms, making text search more effective, scalable, and\nfeature-rich. For those looking to improve the search capabilities of\ntheir apps, the Neo4j-OpenAI-Typesense combination seems to be a strong\noption as businesses struggle with ever-more complicated data\u00a0models.\n\n![](https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=603718bff21e){width=\"1\"\nheight=\"1\"}\n","doi":"https://doi.org/10.59350/1n1ff-8hw48","guid":"https://medium.com/p/603718bff21e","id":"ad5b6f17-1160-48de-8007-b3957e23163b","image":"https://cdn-images-1.medium.com/max/1024/1*3_mdx5SSgImdy1c2RZzl1A.png","indexed_at":1,"language":"en","published_at":1705557442,"reference":[],"relationships":[],"summary":"A novel approach to improving the efficiency of text search in graph databases utilizing Neo4j, OpenAI, and Typesense. Authors: Nakul Nambiar, Aishwarya Nambissan The ability to use cutting-edge tools and frameworks is essential for staying ahead in the ever-changing field of technology.","tags":["Data Science","Neo4j","Knowledge Graph","Typesense","Artificial Intelligence"],"title":"Typesense and Neo4j in a hybrid information retrieval solution","updated_at":1705557442,"url":"https://medium.com/@researchgraph/typesense-and-neo4j-in-a-hybrid-information-retrieval-solution-603718bff21e"},{"abstract":null,"archive_url":null,"authors":[{"name":"Research Graph"}],"blog":{"api":false,"archive_prefix":null,"authors":null,"backlog":0,"canonical_url":null,"category":"computerAndInformationSciences","created_at":1706685423,"current_feed_url":null,"description":"Stories by Research Graph on Medium","favicon":"https://cdn-images-1.medium.com/fit/c/150/150/1*laJi0jBkVoGhXid7gD_DmQ.png","feed_format":"application/rss+xml","feed_url":"https://medium.com/@researchgraph/feed","filter":null,"funding":null,"generator":"Medium","generator_raw":"Medium","home_page_url":"https://medium.com/@researchgraph","id":"30da2ca9-8258-4ab5-acca-3919d9a5d98d","indexed":true,"issn":null,"language":"en","license":"https://creativecommons.org/licenses/by/4.0/legalcode","mastodon":"","plan":"Starter","prefix":"10.59350","relative_url":null,"ror":null,"secure":true,"slug":"researchgraph","status":"active","title":"Stories by Research Graph on Medium","updated_at":1708297648,"use_api":null,"use_mastodon":false,"user_id":"a7e16958-1175-437c-b839-d4b8a47ec811","version":"https://jsonfeed.org/version/1.1"},"blog_name":"Research Graph","blog_slug":"researchgraph","content_text":"A brief overview of different types of clustering techniques and their\nalgorithms.\n\nAuthors: [Aishwarya\nNambissan](https://www.linkedin.com/in/aishwarya-nambissan-127229200/)\u00a0,\n[Amir\u00a0Aryani](https://www.linkedin.com/in/amiraryani/)\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/max/1024/0*ZJM5dYsM_W_NdXm7\" />\n<figcaption>Photo by <a\nhref=\"https://unsplash.com/@santesson89?utm_source=medium&amp;utm_medium=referral\">Andrea\nDe Santis</a> on\u00a0<a\nhref=\"https://unsplash.com?utm_source=medium&amp;utm_medium=referral\">Unsplash</a></figcaption>\n</figure>\n\n### **Background**\n\nClustering is a fascinating technique used in machine learning, where\npatterns or data points are grouped based on their similarities. It's\nlike finding hidden connections among different data points without\npredefined labels. However, clustering comes with its fair share of\nchallenges that make it more complex than supervised classification.\n\nOne major difficulty with clustering is the absence of labelled data.\nUnlike supervised learning, where patterns are already classified,\nclustering requires us to discover similarities without any prior\nknowledge. This lack of labelled data makes the clustering process\ninherently trickier.\n\nMoreover, clustering heavily relies on numeric data to represent pattern\nfeatures. This means that we can only extract information about\nrelationships among patterns through mathematical operations.\nUnfortunately, this limitation hampers the ability of most clustering\nalgorithms to capture intricate relationships or dependencies in\nnon-numeric data.\n\nAnother factor contributing to the complexity of clustering is the\nabsence of a precise definition of a \"cluster.\" Since we don't have\nwell-defined categories or pre-labelled data, the concept of clusters\nbecomes subjective and dependent on the chosen algorithm and its\nparameters.\n\nHowever, different clustering approaches have emerged to address these\nconcerns and make clustering a more robust and versatile technique.\nThese approaches strive to overcome the challenges posed by the lack of\nlabelled data, the numerical nature of the features, and the subjective\nnature of clusters.\n\nThis article is about the most common, advanced and accessible\nclustering techniques. We have classified them based on the different\ncharacteristics of the method, limitations, and evaluation methods.\nThese techniques include partitioning, hierarchical, density-based,\ndistribution-based and spectral clustering to name a\u00a0few.\n\n### The 19 Clustering Techniques\n\nWith a comprehensive list of 19 different clustering techniques, each\nwith its own advantages and disadvantages, you can find the ideal\nalgorithm that suits your specific needs. Additionally, not only do\nthese techniques offer a variety of options, but each also comes with\nmultiple algorithms to choose from. The following is the list of the\nmost common clustering categories. You can think about these as\ndifferent classes of clustering techniques.\n\n<figure>\n<img\nsrc=\"https://cdn-images-1.medium.com/proxy/1*kF7OFCvky-EU83yBwm7mjA.png\" />\n<figcaption>The 19 clustering techniques and their corresponding\nalgorithms</figcaption>\n</figure>\n\n**Partitioning**\n\nWhen you wish to group customers with similar purchasing behaviors,\npreferences, or demographics, helping businesses create targeted\nmarketing strategies i.e. Customer Segmentation, or if you wish to\nperform image recognition, Partitioning Clustering is the way to go.\nThis technique involves dividing a dataset into several disjoint\npartitions or clusters. Each data point is assigned to a specific\ncluster based on certain criteria. For example in k-means clustering,\neach cluster is represented by its center (i.e. *centroid*) which\ncorresponds to the mean of points assigned to the cluster. The clusters\nare defined so that the points within a cluster are similar to each\nother and, different from the data points in another\u00a0cluster.\n\nAlgorithms:\n\n- [K-Means](https://www.javatpoint.com/k-means-clustering-algorithm-in-machine-learning)\n- [K-Medoids](https://www.javatpoint.com/k-medoids-clustering-theoretical-explanation)\n- [PAM](https://towardsdatascience.com/a-deep-dive-into-partitioning-around-medoids-a77d9b888881)\u200a---\u200aPartition\n  Around\u00a0Medoids\n- [CLARA\u200a](https://www.datanovia.com/en/lessons/clara-in-r-clustering-large-applications/)---\u200aClustering\n  Large Applications\n- [CLARANS](https://analyticsindiamag.com/comprehensive-guide-to-clarans-clustering-algorithm/)\u200a---\u200aClustering\n  Large Applications based upon RANdomized Search\n- [AP\u200a](https://medium.com/serpdotai/affinity-propagation-13c43f2fd883)---\u200aAffinity\n  Propagation\n\n*More information about the technique available*\n[*here*](https://medium.com/analytics-vidhya/partitional-clustering-181d42049670)\n\n**Hierarchical**\n\nHierarchical clustering is a popular algorithm used in unsupervised\nmachine learning for dividing a dataset into a hierarchy of clusters. It\naims to organize the data points in a tree-like structure called a\ndendrogram, which represents the relationships between the points based\non their similarities or dissimilarities. Each data point initially\nforms its own cluster, and then the algorithm repeatedly merges the most\nsimilar clusters together until there is only one cluster remaining. The\nsimilarity between clusters is often measured using methods such as\nEuclidean distance or correlation. This technique is great for\nclassifying genes with similar expression patterns or grouping similar\nbiological samples together.\n\nAlgorithms:\n\n- [CURE](https://www.geeksforgeeks.org/basic-understanding-of-cure-algorithm/)\u200a---\u200aClustering\n  Using REpresentatives\n- [ROCK\u200a](https://medium.com/geekculture/the-rock-algorithm-in-machine-learning-5fa152f34de7)---\u200aRObust\n  Clustering using\u00a0LinKs\n- [Chameleon](https://www.tutorialspoint.com/what-is-a-chameleon)\n- [BIRCH](https://medium.com/@noel.cs21/balanced-iterative-reducing-and-clustering-using-heirachies-birch-5680adffaa58)\u200a---\u200aBalanced\n  Iterative Reducing and Clustering using Hierarchies\n\n*More information about the technique available*\n[*here*](https://www.learndatasci.com/glossary/hierarchical-clustering/)\n\n**Density-based**\n\nDensity-based clustering is a technique used to discover clusters in a\ndata collection based on the density of data points in their vicinity.\nIt identifies areas with a higher density of data points compared to the\nsurrounding areas. The clusters are defined as regions of high-density\nseparated by regions of low-density. The points within a cluster are\nuniformly dense, and regions of low-density separate points in different\nclusters. This technique is commonly used in spatial datasets, such as\nGPS coordinates or geographical regions, to find clusters based on\nproximity.\n\nAlgorithms:\n\n- [DBSCAN](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html)\u200a---**\u200a**Density-Based\n  Spatial Clustering of Applications with\u00a0Noise\n- [OPTICS\u200a](https://www.geeksforgeeks.org/ml-optics-clustering-explanation/)---\u200aOrdering\n  Points To Identify Cluster Structure\n- [Mean-shift](https://medium.com/@shruti.dhumne/mean-shift-clustering-a-powerful-technique-for-data-analysis-with-python-f0c26bfb808a)\n- [DENCLUE\u200a](https://www.janbasktraining.com/tutorials/density-based-clustering/)---\u200aDENsity-Based\n  CLUstering\n\n*More information about the technique available*\n[*here*](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/how-density-based-clustering-works.htm)\n\n**Distribution based**\n\nDistribution-based clustering algorithms offer a sophisticated approach\nto analyzing complex datasets where data points follow distinct\nprobability distributions. Unlike traditional clustering methods that\nrely on proximity or density, distribution-based clustering excels in\nidentifying varied cluster shapes and densities, making it particularly\nuseful in datasets with heterogeneous characteristics. These algorithms\noperate on the principle that data points are not randomly scattered but\nadhere to specific statistical patterns, such as normal or Gaussian\ndistributions. The main objective is to detect these underlying\ndistributions in the dataset and group data points into clusters based\non their statistical distribution. This methodology is a departure from\nconventional clustering techniques like K-means, which typically focus\non distance metrics and assume homogeneity in cluster shapes and\u00a0sizes.\n\nIn practical applications, distribution-based clustering is invaluable\nin areas like market research, where consumer behaviors may exhibit\ndifferent patterns, or in biological research for analyzing gene\nexpression data. The process involves assigning each data point to a\ncluster by determining the likelihood of it belonging to the\ndistribution characteristic of that cluster. This advanced technique is\nadept at handling data where clusters differ significantly in size,\nshape, or density, thus providing a more nuanced and accurate clustering\nsolution for complex datasets. The ability to identify and align with\nthe intricate probability distributions within data makes\ndistribution-based clustering a powerful tool in fields requiring\ndetailed data analysis and pattern recognition.\n\nAlgorithms:\n\n- [DBCLASD\u200a](http://10.1109/ICDE.1998.655795)---\u200aDistribution-Based\n  Clustering of LArge Spatial Databases\n- [GMM](https://behesht.medium.com/unsupervised-learning-clustering-using-gaussian-mixture-model-gmm-c788b280932b)\u200a---\u200aGaussian\n  Mixture\u00a0Model\n\n*More information about the technique available*\n[*here*](https://en.wikipedia.org/wiki/Cluster_analysis#Distribution-based_clustering)\n\n**Spectral**\n\nSpectral clustering is suited for complex datasets. This method is\nespecially effective when the data is not straightforward and contains\nnon-linear structures. It goes beyond surface-level analysis, diving\ndeep into the data's core structure to uncover connections and groupings\nthat might not be immediately apparent. This ability to organize complex\ndata patterns is what makes spectral clustering a powerful tool for\nvarious challenging clustering tasks.\n\nSpectral clustering is extensively used in various domains such as\ncomputer vision, image segmentation, community detection in social\nnetworks, and text clustering. The core principle of this method is\nclustering data points based on their similarity, rather than on\ndistance metrics used in methods like\u00a0K-means.\n\nThe spectral clustering technique is fundamentally graph-based. It\nbegins by constructing a similarity matrix, which represents the degree\nof similarity between each pair of data points in the dataset. The\nunique aspect of spectral clustering lies in its utilization of\neigenvectors (or spectra) of this similarity matrix. By analyzing these\neigenvectors, the algorithm effectively partitions the data into\ndistinct clusters. This approach allows for the identification of\nclusters that are connected or grouped based on similarity in a\nhigher-dimensional space, making it highly effective for data that forms\ncomplex shapes or structures. The technique is particularly adept at\nhandling scenarios where the clusters are intertwined or not easily\nseparable by linear boundaries.\n\nIn practical applications, spectral clustering's ability to capture the\nessence of complex relationships within data makes it a popular choice\nfor computer vision and image segmentation as it helps in identifying\nregions of images that are similar in texture or color. In social\nnetwork analysis, it is used for community detection, where it\nidentifies groups of individuals with similar interests or connections.\nSimilarly, in text clustering, spectral clustering aids in grouping\ndocuments or texts that are topically similar, even when the\nrelationship between the texts is not straightforward.\n\nAlgorithms:\n\n- [SM\u200a](https://sites.stat.washington.edu/spectral/papers/UW-CSE-03-05-01.pdf)---\u200aShi\n  and\u00a0Malik\n- [NJW\u200a](https://sites.stat.washington.edu/spectral/papers/UW-CSE-03-05-01.pdf)---\u200aNg,\n  Jordan and\u00a0Weiss\n\n*More information about the technique available*\n[*here*](https://en.wikipedia.org/wiki/Spectral_clustering)\n\n**Fuzzy**\n\nFuzzy clustering is a technique used in data analysis and data mining to\nclassify data points into multiple clusters based on their similarity.\nUnlike traditional clustering algorithms, fuzzy clustering assigns a\ndegree of membership to each data point, indicating the likelihood of\nthat point belonging to each\u00a0cluster.\n\nIn fuzzy clustering, every data point is associated with a membership\nvalue between 0 and 1 for each cluster. These membership values\nrepresent the degree of belongingness to each cluster. A data point can\nhave a higher membership value for one cluster and a lower value for\nothers, making it a more flexible approach compared to hard clustering\ntechniques like\u00a0K-means.\n\nThis algorithms following this technique work iteratively to optimize\nthe memberships and cluster centers. They calculate the distance between\ndata points and cluster centers and update the memberships based on\nthese distances. Smaller the distance, higher the membership. The\niteration continues until a certain convergence criterion is met,\ntypically when the memberships or cluster centers do not significantly\nchange.\n\nSome common applications of fuzzy clustering include market\nsegmentation, image segmentation, pattern recognition, and data\ncompression. It enables a more nuanced understanding of complex datasets\nand provides probabilistic categorization rather than strict\ncategorization into clusters.\n\nAlgorithms:\n\n- [FCM](https://medium.com/geekculture/fuzzy-c-means-clustering-fcm-algorithm-in-machine-learning-c2e51e586fff)\u200a---\u200aFuzzy\u00a0C-Means\n- [FCS\u200a](https://iopscience.iop.org/article/10.1088/1742-6596/1613/1/012006/pdf)---\u200aFuzzy\u00a0C-Shells\n- [MM\u200a](https://doi.org/10.1007/s00500-021-06397-7)---\u200aMeskat-Mahmudul\n\n*More information about the technique is available*\n[*here*](https://www.geeksforgeeks.org/ml-fuzzy-clustering/)\n\n**Graph-theory based**\n\nGraph theory-based clustering approaches data organization from a\ndifferent perspective. It uses the concepts of graphs and networks to\nuncover clusters. The data points are the nodes (vertices) in a graph,\nwhile the connections (edges) represent relationships or similarities\nbetween them. The strength of edges can reflect the degree of similarity\nor connection strength. The nodes within each identified cluster\nrepresent data points that are more closely related to each other than\nto those in other clusters. The edges within clusters are generally\nstronger than those between clusters, indicating tighter connections.\nThis creates a visual representation of the data's structure, revealing\npatterns and community-like structures.\n\nThis technique handles complex relationships by capturing non-linear\nrelationships and patterns that traditional distance-based clustering\nmethods might miss. The graph construction can be tailored to reflect\nspecific domain knowledge about relationships between data points. It\ndiscovers overlapping clusters(soft clustering) representing multiple\nfacets or roles. Therefore, it is highly useful for social network\nanalysis and protein interaction analysis in biological networks.\n\nAlgorithms:\n\n- [CLICK](https://pubmed.ncbi.nlm.nih.gov/10977092)\n- [MST\u200a](https://sites.google.com/site/dataclusteringalgorithms/mst-based-clustering-algorithm)---\u200aMinimum\n  Spanning\u00a0Tree\n- [SM\u200a](https://sites.stat.washington.edu/spectral/papers/UW-CSE-03-05-01.pdf)---\u200aShi\n  and\u00a0Malik\n- [NJW](https://sites.stat.washington.edu/spectral/papers/UW-CSE-03-05-01.pdf)\u200a---\u200aNg,\n  Jordan and\u00a0Weiss\n\n*More information about the technique is available*\n[*here*](https://www.cse.msu.edu/~cse802/S17/slides/Lec_20_21_22_Clustering.pdf)\n\n**Grid based**\n\nGrid-based clustering is a technique used to group data points together\nbased on their spatial proximity in a two-dimensional space. It uses a\nmulti-resolution grid data structure to achieve this. It divides the\nspace into a fixed number of equally sized grids and assigns data points\nto their respective grid cells. This approach provides a simple and\nefficient way to organize and analyze large datasets.\n\nThe first step in this technique is to create a grid over the\ntwo-dimensional space, subdividing it into a predetermined number of\ncells. Each data point is then assigned to its corresponding grid cell\nbased on its location, and the density of each cell is calculated. If\nthe density of the cell is below a certain threshold, it is eliminated\nfrom consideration. The adjacent groups of dense cells form clusters.\n\nGrid-based clustering is especially useful in scenarios where the\nspatial distribution of data is important. It is commonly used in\napplications such as Image Segmentation, Spatial Data Analysis and\nNetwork Traffic Analysis.\n\nAlgorithms:\n\n- [STING\u200a](https://www.tutorialspoint.com/what-is-sting-grid-based-clustering)---\u200aSTatistical\n  INformation Grid\n- [CLIQUE](https://www.geeksforgeeks.org/clique-algorithm-in-data-mining/)\n- [Wavecluster](https://www.vldb.org/conf/1998/p428.pdf)\n\n*More information about the technique is available*\n[*here*](https://nitsri.ac.in/Department/Computer%20Science%20&%20Engineering/BD28.pdf)\n\n**Model-based**\n\nModel-based clustering takes a statistical approach. It assumes that the\ndata points are drawn from a finite combination of component models.\nEach of these component models are probability distribution. It then\nfinds the specific mixture and its parameters that best fit the\u00a0data.\n\nInitially, a suitable model is chosen to represent the data\ndistribution. Commonly used models are Gaussian mixture models, hidden\nMarkov models, or finite mixture models. Then the parameters of the\nchosen model are estimated, typically using an iterative algorithm such\nas Expectation-Maximization (EM). Once the model parameters are\nestimated, each data point is assigned to the cluster that maximizes the\nlikelihood of belonging to that cluster based on the model. The\nassignment of data points to clusters may not be perfect initially, so\nthe process iterates by updating the model parameters and reassigning\ndata points until convergence.\n\nModel-based clustering is used in various domains, such as in\nexploratory data analysis, customer segmentation, image analysis, and\nbioinformatics. It is particularly useful when the data distribution is\ncomplex and requires a probabilistic description for clustering.\nModel-based clustering can identify clusters with different shapes,\nsizes, and densities, offering flexibility compared to other clustering\ntechniques.\n\nAlgorithms:\n\n- [COBWEB](https://doi.org/10.1515/JISYS.2009.18.3.229)\n- [GMM](https://behesht.medium.com/unsupervised-learning-clustering-using-gaussian-mixture-model-gmm-c788b280932b)\u200a---\u200aGaussian\n  Mixture\u00a0Model\n- [SOM\u200a](https://medium.com/machine-learning-researcher/self-organizing-map-som-c296561e2117)---\u200aSelf-Organizing\n  Map\n- [ART](http://10.1109/INNOVATIONS.2008.4781647)\n\n*More information about the technique is available*\n[*here*](https://www.tutorialspoint.com/what-is-model-based-clustering)\n\n**Quantum theory\u00a0based**\n\nQuantum Clustering is a technique used to analyze a set of points in a\nmulti-dimensional space. It represents each point with a Gaussian\ndistribution centred at its location. These Gaussian distributions are\nadded together to create a single distribution for the entire data\u00a0set.\n\nQuantum clustering is used in various scenarios, particularly in complex\ndata clustering problems where traditional clustering algorithms\nstruggle. It has been applied to fields such as image and pattern\nrecognition, bioinformatics, network analysis, and anomaly detection. It\noffers a unique perspective on clustering data by exploiting the\nprinciples of quantum mechanics, aiming to find patterns and\nrelationships that might not be easily discoverable with classical\nmethods.\n\nAlgorithms:\n\n- [QC](https://en.wikipedia.org/wiki/Quantum_clustering)\u200a---\u200aQuantum\n  Clustering\n- [DQC](https://doi.org/10.48550/arXiv.1310.2700)\u200a---\u200aDynamic Quantum\n  Clustering\n\n*More information about the technique is available*\n[*here*](https://doi.org/10.1007/s40745-015-0040-1)\n\n**Affinity Propagation based**\n\nIt is based on the concept of \"message passing\" between data points. It\ndetermines data point representatives, or exemplars, and assigns other\ndata points to these exemplars like in K-Medoids algorithm.\n\nThis technique begins by calculating a similarity matrix between all\npairs of data points. This matrix represents the similarity or\ndissimilarity between each data point. It then iteratively updates two\nmatrices: the responsibility matrix and the availability matrix. The\nresponsibility matrix contains a measure of the accumulated evidence\nthat one data point should be an exemplar for another, while the\navailability matrix contains a measure of the accumulated evidence that\na particular data point should choose another as its exemplar. AP\nperforms multiple rounds of message passing between data points to\nupdate the responsibility and availability matrices. During each round,\neach data point sends its current values to all others, resulting in a\nseries of message exchanges. After convergence, each data point selects\nthe exemplar with the highest net responsibility for itself as its\nexemplar. Finally, AP assigns each data point to its corresponding\nexemplar, resulting in the formation of clusters.\n\nIt is a versatile clustering algorithm suitable for many applications\nwhere the number of clusters is not known in advance, and it allows for\nautomatic identification of exemplars. It is used in scenarios such as\nImage Segmentation, Document Clustering, Gene Expression Analysis and\nSocial Network Analysis.\n\nAlgorithms:\n\n- [AP](https://medium.com/serpdotai/affinity-propagation-13c43f2fd883)\u200a---\u200aAffinitity\n  Propagation\n\n*More information about the technique available*\n[*here*](https://en.wikipedia.org/wiki/Affinity_propagation)\n\n**Spatial Clustering**\n\nSpatial clustering is a technique used in data mining and geographic\ninformation systems (GIS) to classify and group spatial data(any type of\ndata that directly or indirectly references a specific geographical area\nor location) into clusters based on their proximity to each other. It\ninvolves grouping similar objects that are close to each other in\u00a0space.\n\nIn spatial clustering, a distance or similarity measure is used to\ndetermine the proximity between spatial objects. It can be based on\nEuclidean distance, Manhattan distance, or any other suitable metric.\nSeveral algorithms are available to perform spatial clustering, such as\nk-means, hierarchical clustering, density-based clustering (DBSCAN),\netc. These algorithms employ different techniques for grouping objects\nbased on their proximity. The algorithm is applied to the spatial data,\nand objects are assigned to different clusters based on their similarity\nand proximity to other objects. The goal is to maximize the similarity\nwithin clusters while maximizing the dissimilarity between clusters.\n\nSpatial clustering is used in various domains and applications,\nincluding Geographic analysis where it helps identify spatial patterns,\nhotspots, and areas with high or low concentrations of specific\nphenomena, pattern recognition and urban planning and transportation.\n\nAlgorithms:\n\n- [DBSCAN](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html)\u200a---\u200aDensity-Based\n  Spatial Clustering of Applications with\u00a0Noise\n- [STING](https://www.tutorialspoint.com/what-is-sting-grid-based-clustering)\u200a---\u200aSTatistical\n  INformation Grid\n- [Wavecluster](https://www.vldb.org/conf/1998/p428.pdf)\n- [CLARANS](https://analyticsindiamag.com/comprehensive-guide-to-clarans-clustering-algorithm/)\u200a---\u200aClustering\n  Large Applications based upon RANdomized Search\n\n*More information about the technique is available*\n[*here*](https://kazumatsuda.medium.com/spatial-clustering-fa2ea5d035a3)\n\n**Data stream Clustering**\n\nData stream clustering is a technique used to analyze and group data\npoints in real-time as they arrive in a stream. It is commonly employed\nin scenarios where data is continuously generated and needs to be\nprocessed online without storing the entire data\u00a0set.\n\nThe working principle behind data stream clustering involves capturing a\nfixed-size window of incoming data points and performing clustering\nalgorithms on this window. The window is updated by sliding it forward\nwhen new data points arrive and removing the oldest points. As new data\npoints are added to the window, the clustering algorithm updates the\nexisting clusters or creates new\u00a0ones.\n\nData stream clustering is used in numerous domains, including online\nmarketing, fraud detection, network traffic analysis, sensor networks,\nanomaly detection, and social network analysis. It has applications in\ntasks like real-time recommendations, continuous monitoring, pattern\nrecognition, and exploring evolving trends in streaming data.\n\nAlgorithms:\n\n- [STREAM](https://doi.org/10.1007/s10462-013-9398-7)\n- [HPStream](https://doi.org/10.1186/s41044-016-0011-3)\u200a---\u200aHigh-dimensional\n  Projected Stream\n- [CluStream](https://sybernix.medium.com/clustream-a-framework-for-clustering-evolving-data-streams-b2f8b2d65ae)\n- [DenStream](https://doi.org/10.1137/1.9781611972764.29)\u200a---\u200aDensity-based\n  data\u00a0Stream\n\n*More information about the technique is available*\n[*here*](https://en.wikipedia.org/wiki/Data_stream_clustering)\n\n***There is a scarcity of online literature on the following techniques,\nas they pertain to relatively specific and less commonly used clustering\noptions.***\n\n**Fractal-theory based**\n\nFractals are complex geometric shapes exhibiting self-similarity,\nmeaning their patterns repeat at different scales. They are useful in\nmodelling structures (such as snowflakes) in which similar patterns\nrecur at progressively smaller scales. The complexity of fractals is\ncharacterized by \"fractal dimension\". Fractal dimension goes beyond the\nusual dimensions (like length, width, and height) to capture how\ndetailed and \"crinkled\" something is. For example, in a snowflake,\nsimilar patterns repeat at smaller and smaller scales. This\nself-similarity is what distinguishes fractals and defines their fractal\ndimension.\n\nThe underlying principle of fractals is that a simple process that goes\nthrough infinitely many iterations becomes a very complex process.\nFractals attempt to model the complex process by searching for the\nsimple process underneath. Most fractals operate on the principle of a\nfeedback loop. The primary concept behind fractal clustering is to\narrange points within a cluster in a manner that does not significantly\nalter the cluster's intrinsic dimension. Each data point is assigned a\nfractal dimension using techniques like box-counting. Points with\nsimilar fractal dimensions tend to exhibit similar internal patterns,\nsuggesting they belong to the same cluster. Clustering is then done\nusing other techniques such as grid-based clustering where clusters are\nformed by grouping points with similar fractal dimensions within each\ngrid\u00a0cell.\n\nFractal-theory based clustering works well with complex data shapes and\nnon-linear relationships and is efficient for large datasets due to its\ngrid-based nature. It is used for medical image analysis, financial data\nanalysis and biological data analysis.\n\nAlgorithms:\n\n- [FC](https://doi.org/10.1007/0-387-25465-X_28)\u200a---\u200aFractal Clustering\n\n*More information about the technique available*\n[here](https://doi.org/10.1109/CSBW.2005.66)\n\n[**Kernel based**](http://10.1109/CSBW.2005.66)\n\nConsider your data points like dots on a flat surface. Traditional\nclustering methods might struggle to group them effectively if they form\ncurvy clusters or spiral shapes. Kernel-based clustering takes your data\nto a higher-dimensional space where these clusters become more easily\nidentifiable. Kernel-based clustering is thus a powerful technique for\ngrouping data points when your data exhibits complex, non-linear\nrelationships.\n\nKernel methods owe their name to the use of kernel functions, which\nenable them to operate in a high-dimensional, *implicit* feature space\nwithout ever computing the coordinates of the data in that space, but\nrather by simply computing the inner products between the images of all\npairs of data in the feature space. This operation is often\ncomputationally cheaper than the explicit computation of the\ncoordinates. This approach is called the \"**kernel\u00a0trick**\".\n\nA kernel function takes your data points from the original flat space\nand maps them to a new, higher-dimensional space. This transformation\nhappens implicitly, without you needing to calculate the complex new\ncoordinates directly. The key thing is that the kernel captures the\nsimilarities between data points in a way that accounts for their\nnon-linear relationships. Once in the high-dimensional space, any\nstandard clustering algorithm like K-means can be applied, to find\ngroups of data points that are close together. These groups in the new\nspace correspond to actual clusters in the original data, but their\nshapes might be much more complex now. So, after clustering in the new\nspace, you map the data points back to the original space. This brings\nyour clusters back to the real world while retaining their\ncomplex\u00a0shapes.\n\nCommonly used applications for this technique are bioinformatics,\nanomaly detection and gene expression analysis.\n\nAlgorithms:\n\n- [Kernel K-Means](https://doi.org/10.48550/arXiv.2011.06461)\n- [Kernel SOM](https://doi.org/10.1016/j.neucom.2005.10.003)\n- [Kernel FCM](https://doi.org/10.3233/IDT-210091)\n- [SVC\u200a](https://dl.acm.org/doi/10.5555/944790.944807)---\u200aSupport Vector\n  Clustering\n- [MMC](http://luthuli.cs.uiuc.edu/~daf/courses/Opt-2017/Papers/max_margin_clustering.pdf)\u200a---\u200aMaximum\n  Margin Clustering\n- [MKC\u200a](https://doi.org/10.48550/arXiv.2207.06041)---\u200aMultiple Kernel\n  Clustering\n\n*More information about the technique is available*\n[*here*](https://doi.org/10.1007/s40745-015-0040-1)\n\n**Ensemble based**\n\nEnsemble clustering, also called consensus clustering aims to combine\nmultiple base clustering algorithms to produce more consistent, reliable\nand accurate clustering results compared with the individual clustering\nalgorithms. It leverages the idea that different algorithms may have\ndifferent strengths and weaknesses, and by combining their outcomes, a\nmore accurate and robust clustering solution can be obtained.\n\nTo perform this, multiple clustering algorithms are applied to the\ndataset resulting in multiple different clusterings. Each individual\nclustering represents a potential partitioning of the data points into\nclusters. The individual clusterings are then combined using an ensemble\ntechnique, such as majority voting, weighted voting, or consensus\nfunctions. These techniques assign weights to the individual clusterings\nbased on their quality or similarity and produce a final integrated\nclustering solution. Once the ensemble combination step is complete, a\nfinal clustering solution is obtained. This solution represents an\naggregation of the individual clusterings and should ideally capture the\nmost accurate and stable clustering structure of the\u00a0dataset.\n\nEnsemble-based clustering is particularly useful in scenarios where the\ndataset is complex, high-dimensional, or noisy. It can handle a wide\nrange of data types, including numerical, categorical, and mixed data.\nEnsemble-based clustering is applied when one wants to achieve better\nclustering performance and robustness by combining the outcomes of\nmultiple clustering algorithms or multiple runs. It is also valuable\nwhen there is uncertainty in selecting the most appropriate clustering\nalgorithm or setting parameters for a single algorithm.\n\nAlgorithms:\n\nMixture of algorithms based on\u00a0trials\n\n*More information about the technique is available*\n[*here*](https://doi.org/10.1007/s13042-017-0756-7)\n\n**Swarm Intelligence based**\n\nIt is a clustering technique inspired by the collective behaviour of\nsocial insect colonies, specifically the behaviour of swarms. It models\nthe clustering process based on the principles of self-organization and\nemergence observed in natural swarm systems like bees, ants, or\u00a0birds.\n\nIt can be used to form clusters in large-scale sensor networks, enabling\nefficient data aggregation and management or it can be adapted to solve\noptimization problems like facility location, vehicle routing, or graph\npartitioning. It is also used for applications such as data clustering\nand image segmentation.\n\nAlgorithms:\n\n- [ACO Based](https://doi.org/10.1007/11839088_31)\u200a---\u200aAnt Colony\n  Optimization for Clustering\n- [PSO Based](http://10.1109/CEC.2003.1299577)\u200a---\u200aParticle Swarm\n  Optimization\n- [SFLA\n  Based\u200a](https://www.linkedin.com/pulse/shuffled-frog-leaping-algorithm-innovative-approach-problem-chan/)---\u200aShuffled\n  Leaping Frog Algorithm\n- [ABC Based](https://doi.org/10.1016/j.asoc.2009.12.025)\u200a---\u200aArtificial\n  Bee\u00a0Colony\n\n*More information about the technique is available*\n[*here*](https://doi.org/10.1007/978-0-387-69935-6_12)\n\n[**Density and\ndistance-based**](https://doi.org/10.1007/s40745-015-0040-1)\n\nDensity and distance-based clustering is a technique introduced in a\nscientific paper in 2014. Its main concept is unique, and its defining\ncharacteristic is the way it describes cluster centers. According to\nthis technique, a cluster center must meet two criteria:\n\n1.  It should be surrounded by a significant number of data points\n    within a specific range, indicating a high local\u00a0density\n2.  It should be distant from other data points that have a high local\n    density and could potentially be cluster centers themselves.\n\nTo implement this approach, the local density of each data point is\ncalculated and the shortest distance between each data point and others\nwith a higher local density is determined. This information is used to\ncreate a decision graph, which is then used to identify cluster centers.\nThe remaining data points are assigned to the nearest cluster with a\nhigher local\u00a0density.\n\nThis technique is used for data analysis and pattern recognition.\n\nAlgorithms:\n\n- [DD](https://doi.org/10.1007/s40745-015-0040-1)\u200a---\u200aDensity\n  and\u00a0Distance\n\n*More information about the technique is available*\n[*here*](https://doi.org/10.1007/s40745-015-0040-1)\n\n**For large-scale data**\n\nClustering for large scale data presents unique challenges. Traditional\nclustering techniques become computationally expensive with increasing\ndata size, taking longer to execute and requiring more resources.\nHolding massive datasets in memory while clustering might not be\nfeasible, leading to out-of-memory errors and hindering the process.\nTechniques need to be scalable to handle ever-growing datasets without\nsacrificing accuracy or performance. Large datasets often contain more\nnoise and outliers, which can negatively impact clustering results if\nnot effectively handled. To handle these concerns, specific techniques\nand tactics are required to effectively group data points without being\noverwhelmed by the huge\u00a0volume.\n\nApproaches for Large-Scale Clustering:\n\n- Sampling: Instead of analyzing the entire dataset, representative\n  samples are chosen for clustering, reducing computational burden while\n  maintaining acceptable accuracy.\n- Streaming algorithms: Data is processed one point at a time, avoiding\n  holding the entire dataset in memory and adapting to continuously\n  arriving\u00a0data.\n- Parallelization: Dividing the workload across multiple processors or\n  machines can significantly speed up the clustering process for large\n  datasets.\n- Approximation algorithms: These techniques trade-off exactness for\n  efficiency, offering approximate but fast clustering solutions for\n  large\u00a0data.\n- Hierarchical clustering: Grouping data into a hierarchy of clusters\n  can be more efficient than flat clustering for large datasets,\n  allowing for exploration at different granularities.\n\nPopular Large-Scale Clustering Techniques include K-means Clustering\nwith Mini-batching, Streaming K-means\u00a0etc.\n\nA successful large-scale clustering project requires careful selection\nof an appropriate technique, efficient implementation, and consideration\nof the trade-offs between accuracy, performance, and resource\nconstraints.\n\nAlgorithms:\n\n- [BIRCH](https://medium.com/@noel.cs21/balanced-iterative-reducing-and-clustering-using-heirachies-birch-5680adffaa58)\u200a---\u200aBalanced\n  Iterative Reducing and Clustering using Hierarchies\n- [CLARA](https://www.datanovia.com/en/lessons/clara-in-r-clustering-large-applications/#google_vignette)\u200a---\u200aClustering\n  Large Applications\n- [CURE](https://www.geeksforgeeks.org/basic-understanding-of-cure-algorithm/)\u200a---\u200aClustering\n  Using REpresentatives\n- [DBSCAN](https://www.kdnuggets.com/2020/04/dbscan-clustering-algorithm-machine-learning.html)\u200a---\u200aDensity-Based\n  Spatial Clustering of Applications with\u00a0Noise\n- [DENCLUE](https://www.janbasktraining.com/tutorials/density-based-clustering/)\u200a---\u200aDENsity\n  Based CLUstering\n- [Wavecluster](https://www.vldb.org/conf/1998/p428.pdf)\n- [FC](https://doi.org/10.1007/0-387-25465-X_28)\u200a---\u200aFractal Clustering\n- [K-Means](https://www.javatpoint.com/k-means-clustering-algorithm-in-machine-learning)\n\n*More information about the technique is available*\n[*here*](https://doi.org/10.1007/s40745-015-0040-1)\n\n***To access the table of clustering techniques, algorithms and their\nevaluation criteria, please follow the\u00a0link:***\n\n[Excel/README.md at main \u00b7\ndata-community-of-practice/Excel](https://github.com/data-community-of-practice/Excel/blob/main/README.md)\n\n### **Conclusion**\n\nClustering is a fascinating method in the world of machine learning. It\nallows data scientists to uncover hidden patterns within their data\nwithout the need for labelled examples. However, it can be quite\ndifficult to master all relevant techniques.\n\nThe article provides a comprehensive overview of 19 different clustering\ntechniques, categorized by their characteristics and applications. It\nshowcases popular methods like partitioning, hierarchical,\ndensity-based, distribution-based, and spectral clustering. More\nadvanced techniques like fuzzy, graph-theory based, grid-based,\nmodel-based, and quantum clustering are also explored, highlighting\ntheir unique capabilities for handling complex data structures and\nrelationships. Each technique has its own pros and cons and is suitable\nfor different types of data and applications. They can be used in a wide\nrange of fields, including marketing, biology, spatial data analysis,\nand complex data analysis tasks. These advancements are opening up\nexciting possibilities for researchers and data scientists, enabling\nthem to dig deeper into their data and unlock its true potential.\n\nIt is important to consider the specific requirements and\ncharacteristics of the data when selecting a clustering technique. There\nare various algorithms within each clustering technique category\nproviding a range of options for different use cases. If there is\nconfusion when deciding upon the most suitable clustering algorithm for\nyour requirements, our informative table can be referred to discover the\nideal algorithm that resolves your distinctive data challenges.\n\n![](https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=cd5dc3187f0d){width=\"1\"\nheight=\"1\"}\n","doi":"https://doi.org/10.59350/gpqez-x0t82","guid":"https://medium.com/p/cd5dc3187f0d","id":"33e06ba9-de54-452a-b04c-6ece3fa9bef1","image":"https://cdn-images-1.medium.com/max/1024/0*ZJM5dYsM_W_NdXm7","indexed_at":1,"language":"en","published_at":1705006239,"reference":[],"relationships":[],"summary":"A brief overview of different types of clustering techniques and their algorithms. Authors: Aishwarya Nambissan\u00a0, Amir\u00a0Aryani\n<strong>\n <strong>\n  Background\n </strong>\n</strong>\nClustering is a fascinating technique used in machine learning, where patterns or data points are grouped based on their similarities. It\u2019s like finding hidden connections among different data points without predefined labels.","tags":["Clustering Algorithm","Data Science","Machine Learning","Artificial Intelligence"],"title":"19 Clustering Techniques","updated_at":1705006239,"url":"https://medium.com/@researchgraph/19-clustering-techniques-cd5dc3187f0d"}],"total-results":7}
